{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JayanthK99/Jayanth_INFO5731_Spring2025/blob/main/Kaddipudi_Jayanth_Assignment_02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ryk8D1Q4Wsrp"
      },
      "source": [
        "# **INFO5731 Assignment 2**\n",
        "\n",
        "In this assignment, you will work on gathering text data from an open data source via web scraping or API. Following this, you will need to clean the text data and perform syntactic analysis on the data. Follow the instructions carefully and design well-structured Python programs to address each question.\n",
        "\n",
        "**Expectations**:\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "* **Make sure to submit the cleaned data CSV in the comment section - 10 points**\n",
        "\n",
        "**Total points**: 100\n",
        "\n",
        "**Deadline**: Monday, at 11:59 PM.\n",
        "\n",
        "**Late Submission will have a penalty of 10% reduction for each day after the deadline.**\n",
        "\n",
        "**Please check that the link you submitted can be opened and points to the correct assignment.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkzR8cFAyGik"
      },
      "source": [
        "# Question 1 (25 points)\n",
        "\n",
        "Write a python program to collect text data from **either of the following sources** and save the data into a **csv file:**\n",
        "\n",
        "(1) Collect all the customer reviews of a product (you can choose any porduct) on amazon. [atleast 1000 reviews]\n",
        "\n",
        "(2) Collect the top 1000 User Reviews of a movie recently in 2023 or 2024 (you can choose any movie) from IMDB. [If one movie doesn't have sufficient reviews, collect reviews of atleast 2 or 3 movies]\n",
        "\n",
        "\n",
        "(3) Collect the **abstracts** of the top 10000 research papers by using the query \"machine learning\", \"data science\", \"artifical intelligence\", or \"information extraction\" from Semantic Scholar.\n",
        "\n",
        "(4) Collect all the information of the 904 narrators in the Densho Digital Repository.\n",
        "\n",
        "(5)**Collect a total of 10000 reviews** of the top 100 most popular software from G2 and Capterra.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "from urllib.request import Request, urlopen\n",
        "import csv\n",
        "\n",
        "main_url = \"https://ddr.densho.org/narrators/?page={}\"\n",
        "\n",
        "def extract_narrator_info():\n",
        "    all_narrators_data = []\n",
        "\n",
        "    for page_num in range(1, 42):\n",
        "        link1 = Request(main_url.format(page_num), headers={'User-Agent': 'Mozilla/5.0'})\n",
        "        url1 = urlopen(link1)\n",
        "        data1 = url1.read()\n",
        "        data1_soup = BeautifulSoup(data1, 'html.parser')\n",
        "\n",
        "        for narrator_link in data1_soup.find_all('h4'):\n",
        "            link2 = Request(narrator_link.a.get('href'), headers={'User-Agent': 'Mozilla/5.0'})\n",
        "            url2 = urlopen(link2)\n",
        "            data2 = url2.read()\n",
        "            data2_soup = BeautifulSoup(data2, 'html.parser')\n",
        "\n",
        "            narrator = data2_soup.find(\"div\", attrs={'class': 'col-sm-8 col-md-8'})\n",
        "            narrator_name = narrator.h1.text.strip().replace('\"', \"\")\n",
        "            narrator_bio = narrator.p.text.strip() if narrator.p else \"\"\n",
        "\n",
        "            interview_titles = []\n",
        "            dates_locations = []\n",
        "            densho_ids = []\n",
        "\n",
        "            for interview in data2_soup.find_all(\"div\", attrs={'class': 'media'}):\n",
        "                interview_title = interview.find(\"b\", attrs={'class': 'media-heading'}).text.strip()\n",
        "                interview_details = interview.find(\"div\", attrs={'class': 'source muted'})\n",
        "                date_location = \"\"\n",
        "\n",
        "                if interview_details:\n",
        "                    details_text = interview_details.get_text(\"\\n\").strip().split(\"\\n\")\n",
        "                    if len(details_text) >= 2:\n",
        "                        date = details_text[0].strip()\n",
        "                        location = details_text[1].strip()\n",
        "                        date_location = f\"{date}, {location}\"\n",
        "\n",
        "                interview_link = interview.find(\"a\", href=True)\n",
        "                densho_id = interview_link['href'].split('/')[-2] if interview_link else \"\"\n",
        "\n",
        "                interview_titles.append(interview_title)\n",
        "                dates_locations.append(date_location)\n",
        "                densho_ids.append(densho_id)\n",
        "\n",
        "            all_narrators_data.append({\n",
        "                'Narrator Name': narrator_name,\n",
        "                'Bio': narrator_bio,\n",
        "                'Interview Title': \", \".join(interview_titles),\n",
        "                'Date & Location': \", \".join(dates_locations),\n",
        "                'Densho ID': \", \".join(densho_ids)\n",
        "            })\n",
        "\n",
        "    csv_file = \"List_of_narrators.csv\"\n",
        "    with open(csv_file, mode='w', newline='', encoding='utf-8') as file:\n",
        "        writer = csv.DictWriter(file, fieldnames=[\n",
        "            'Narrator Name', 'Bio', 'Interview Title', 'Date & Location', 'Densho ID'\n",
        "        ])\n",
        "        writer.writeheader()\n",
        "        writer.writerows(all_narrators_data)\n",
        "\n",
        "    print(f\"All narrators' data saved to '{csv_file}'.\")\n",
        "\n",
        "extract_narrator_info()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "6T6nF9bbEzMR",
        "outputId": "9684f29b-1235-4240-f3ad-4a75d6e8cc62"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All narrators' data saved to 'List_of_narrators.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90_NR8c5XGWc"
      },
      "source": [
        "# Question 2 (15 points)\n",
        "\n",
        "Write a python program to **clean the text data** you collected in the previous question and save the clean data in a new column in the csv file. The data cleaning steps include: [Code and output is required for each part]\n",
        "\n",
        "(1) Remove noise, such as special characters and punctuations.\n",
        "\n",
        "(2) Remove numbers.\n",
        "\n",
        "(3) Remove stopwords by using the stopwords list.\n",
        "\n",
        "(4) Lowercase all texts\n",
        "\n",
        "(5) Stemming.\n",
        "\n",
        "(6) Lemmatization."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#(1) Remove noise, such as special characters and punctuations.\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "file_path = \"/content/List_of_narrators.csv\"\n",
        "output_file_path = \"/content/Step_1_List_of_narrators.csv\"\n",
        "\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "def clean_text(text):\n",
        "    if isinstance(text, str):\n",
        "        text = re.sub(r'\\|+', ' ', text)\n",
        "        text = re.sub(r'[^\\w\\s]', '', text)\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "df['Bio'] = df['Bio'].astype(str).apply(clean_text)\n",
        "df['Interview Title'] = df['Interview Title'].astype(str).apply(clean_text)\n",
        "df['Date & Location'] = df['Date & Location'].astype(str).apply(clean_text)\n",
        "df['Densho ID'] = df['Densho ID'].astype(str).apply(clean_text)\n",
        "df.to_csv(output_file_path, index=False)\n",
        "df_cleaned = pd.read_csv(output_file_path)\n",
        "df_cleaned.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "collapsed": true,
        "id": "hEBsQtgZ-WXW",
        "outputId": "f535c6ae-6f3c-494a-b4c4-12ffa7e75bd2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           Narrator Name                                                Bio  \\\n",
              "0           Kay Aiko Abe  Nisei female Born May 9 1927 in Selleck Washin...   \n",
              "1                Art Abe  Nisei male Born June 12 1921 in Seattle Washin...   \n",
              "2  Sharon Tanagi Aburano  Nisei female Born October 31 1925 in Seattle W...   \n",
              "3        Toshiko Aiboshi  Nisei female Born July 8 1928 in Boyle Heights...   \n",
              "4      Douglas L. Aihara  Sansei male Born March 15 1950 in Torrance Cal...   \n",
              "\n",
              "                                     Interview Title  \\\n",
              "0            Kay Aiko Abe Interview ddrdensho1000232   \n",
              "1                 Art Abe Interview ddrdensho1000206   \n",
              "2  Sharon Tanagi Aburano Interview II ddrdensho10...   \n",
              "3              Toshiko Aiboshi Interview ddrmanz1112   \n",
              "4        Douglas L Aihara Interview ddrdensho1000522   \n",
              "\n",
              "                                     Date & Location  \\\n",
              "0                 December 2 2008 Seattle Washington   \n",
              "1                 January 24 2008 Seattle Washington   \n",
              "2  April 3 2008 Seattle Washington March 25 2008 ...   \n",
              "3             January 20 2011 Culver City California   \n",
              "4                      9Nov22 Los Angeles California   \n",
              "\n",
              "                           Densho ID  \n",
              "0                   ddrdensho1000232  \n",
              "1                   ddrdensho1000206  \n",
              "2  ddrdensho1000209 ddrdensho1000208  \n",
              "3                        ddrmanz1112  \n",
              "4                   ddrdensho1000522  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a3d8fd9d-78a2-4b86-aeb2-0fd3acbe3c23\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Narrator Name</th>\n",
              "      <th>Bio</th>\n",
              "      <th>Interview Title</th>\n",
              "      <th>Date &amp; Location</th>\n",
              "      <th>Densho ID</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Kay Aiko Abe</td>\n",
              "      <td>Nisei female Born May 9 1927 in Selleck Washin...</td>\n",
              "      <td>Kay Aiko Abe Interview ddrdensho1000232</td>\n",
              "      <td>December 2 2008 Seattle Washington</td>\n",
              "      <td>ddrdensho1000232</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Art Abe</td>\n",
              "      <td>Nisei male Born June 12 1921 in Seattle Washin...</td>\n",
              "      <td>Art Abe Interview ddrdensho1000206</td>\n",
              "      <td>January 24 2008 Seattle Washington</td>\n",
              "      <td>ddrdensho1000206</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Sharon Tanagi Aburano</td>\n",
              "      <td>Nisei female Born October 31 1925 in Seattle W...</td>\n",
              "      <td>Sharon Tanagi Aburano Interview II ddrdensho10...</td>\n",
              "      <td>April 3 2008 Seattle Washington March 25 2008 ...</td>\n",
              "      <td>ddrdensho1000209 ddrdensho1000208</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Toshiko Aiboshi</td>\n",
              "      <td>Nisei female Born July 8 1928 in Boyle Heights...</td>\n",
              "      <td>Toshiko Aiboshi Interview ddrmanz1112</td>\n",
              "      <td>January 20 2011 Culver City California</td>\n",
              "      <td>ddrmanz1112</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Douglas L. Aihara</td>\n",
              "      <td>Sansei male Born March 15 1950 in Torrance Cal...</td>\n",
              "      <td>Douglas L Aihara Interview ddrdensho1000522</td>\n",
              "      <td>9Nov22 Los Angeles California</td>\n",
              "      <td>ddrdensho1000522</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a3d8fd9d-78a2-4b86-aeb2-0fd3acbe3c23')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a3d8fd9d-78a2-4b86-aeb2-0fd3acbe3c23 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a3d8fd9d-78a2-4b86-aeb2-0fd3acbe3c23');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-7f6ab0db-921d-46f3-96ef-8c9f40097be5\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7f6ab0db-921d-46f3-96ef-8c9f40097be5')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-7f6ab0db-921d-46f3-96ef-8c9f40097be5 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_cleaned",
              "summary": "{\n  \"name\": \"df_cleaned\",\n  \"rows\": 1002,\n  \"fields\": [\n    {\n      \"column\": \"Narrator Name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1000,\n        \"samples\": [\n          \"Masahiro Nakajo\",\n          \"Fred Tadashi Shingu\",\n          \"Fred Shiosaki\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Bio\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 989,\n        \"samples\": [\n          \"Nisei female Born March 3 1918 in Portland Oregon Grew up in Independence Oregon where parents ran a farm Married and was pregnant with first child before World War II During the war was removed to the Portland Assembly Center Oregon and the Minidoka concentration camp Idaho After leaving camp returned to the Gresham Oregon area\",\n          \"Sansei male Born December 1 1941 in San Jose California Removed as an infant with parents to the Heart Mountain concentration camp Wyoming After leaving camp family resettled in Illinois and Gerald eventually attended the University of Illinois and then moved to the East Coast Became involved in civil rights organizations\",\n          \"Born August 20 1938 in Lima Peru Parents ran a laundry business and father was a prominent community leader During World War II the FBI arrested Georges father and the entire family was placed on a ship and sent to the Department of Justice camp at Crystal City Texas After leaving camp the family was not allowed to return to Peru so they moved to San Francisco with the help and sponsorship of a Shinto church reverend Became involved in numerous activities such as Boy Scouts and basketball Drafted into the army and served in Germany\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Interview Title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 961,\n        \"samples\": [\n          \"Richard Konda Interview ddrjamsj28\",\n          \"Dorothy Kuwaye Interview ddrmanz168\",\n          \"Michiko Kornhauser Interview ddrone725\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Date & Location\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 739,\n        \"samples\": [\n          \"October 25 2011 Los Angeles California\",\n          \"June 16 2009 Bloomington Minnesota\",\n          \"September 6 2019 Chicago Illinois\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Densho ID\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 961,\n        \"samples\": [\n          \"ddrjamsj28\",\n          \"ddrmanz168\",\n          \"ddrone725\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#(2) Remove numbers.\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "input_file = \"/content/Step_1_List_of_narrators.csv\"\n",
        "output_file = \"/content/Step_2_List_of_narrators.csv\"\n",
        "def remove_numbers(text):\n",
        "    if isinstance(text, str):\n",
        "        text = re.sub(r'\\d+', '', text)\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "df = pd.read_csv(input_file, dtype=str)\n",
        "df = df.applymap(remove_numbers)\n",
        "df.to_csv(output_file, index=False)\n",
        "df_cleaned = pd.read_csv(output_file)\n",
        "df_cleaned.head()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327
        },
        "collapsed": true,
        "id": "Mb7WqHbabSeL",
        "outputId": "ed09f939-c4d8-4fc9-da6c-7dfbe178926a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-1fd3b17334aa>:13: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  df = df.applymap(remove_numbers)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           Narrator Name                                                Bio  \\\n",
              "0           Kay Aiko Abe  Nisei female Born May in Selleck Washington Sp...   \n",
              "1                Art Abe  Nisei male Born June in Seattle Washington Gre...   \n",
              "2  Sharon Tanagi Aburano  Nisei female Born October in Seattle Washingto...   \n",
              "3        Toshiko Aiboshi  Nisei female Born July in Boyle Heights Califo...   \n",
              "4      Douglas L. Aihara  Sansei male Born March in Torrance California ...   \n",
              "\n",
              "                                     Interview Title  \\\n",
              "0                   Kay Aiko Abe Interview ddrdensho   \n",
              "1                        Art Abe Interview ddrdensho   \n",
              "2  Sharon Tanagi Aburano Interview II ddrdensho S...   \n",
              "3                  Toshiko Aiboshi Interview ddrmanz   \n",
              "4               Douglas L Aihara Interview ddrdensho   \n",
              "\n",
              "                                     Date & Location            Densho ID  \n",
              "0                        December Seattle Washington            ddrdensho  \n",
              "1                         January Seattle Washington            ddrdensho  \n",
              "2  April Seattle Washington March Seattle Washington  ddrdensho ddrdensho  \n",
              "3                     January Culver City California              ddrmanz  \n",
              "4                         Nov Los Angeles California            ddrdensho  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4471bc09-067e-491d-8c27-c9acc8f6085a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Narrator Name</th>\n",
              "      <th>Bio</th>\n",
              "      <th>Interview Title</th>\n",
              "      <th>Date &amp; Location</th>\n",
              "      <th>Densho ID</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Kay Aiko Abe</td>\n",
              "      <td>Nisei female Born May in Selleck Washington Sp...</td>\n",
              "      <td>Kay Aiko Abe Interview ddrdensho</td>\n",
              "      <td>December Seattle Washington</td>\n",
              "      <td>ddrdensho</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Art Abe</td>\n",
              "      <td>Nisei male Born June in Seattle Washington Gre...</td>\n",
              "      <td>Art Abe Interview ddrdensho</td>\n",
              "      <td>January Seattle Washington</td>\n",
              "      <td>ddrdensho</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Sharon Tanagi Aburano</td>\n",
              "      <td>Nisei female Born October in Seattle Washingto...</td>\n",
              "      <td>Sharon Tanagi Aburano Interview II ddrdensho S...</td>\n",
              "      <td>April Seattle Washington March Seattle Washington</td>\n",
              "      <td>ddrdensho ddrdensho</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Toshiko Aiboshi</td>\n",
              "      <td>Nisei female Born July in Boyle Heights Califo...</td>\n",
              "      <td>Toshiko Aiboshi Interview ddrmanz</td>\n",
              "      <td>January Culver City California</td>\n",
              "      <td>ddrmanz</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Douglas L. Aihara</td>\n",
              "      <td>Sansei male Born March in Torrance California ...</td>\n",
              "      <td>Douglas L Aihara Interview ddrdensho</td>\n",
              "      <td>Nov Los Angeles California</td>\n",
              "      <td>ddrdensho</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4471bc09-067e-491d-8c27-c9acc8f6085a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-4471bc09-067e-491d-8c27-c9acc8f6085a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-4471bc09-067e-491d-8c27-c9acc8f6085a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-c39d1532-5284-4740-8341-6f87798d2563\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c39d1532-5284-4740-8341-6f87798d2563')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-c39d1532-5284-4740-8341-6f87798d2563 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_cleaned",
              "summary": "{\n  \"name\": \"df_cleaned\",\n  \"rows\": 1002,\n  \"fields\": [\n    {\n      \"column\": \"Narrator Name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1000,\n        \"samples\": [\n          \"Masahiro Nakajo\",\n          \"Fred Tadashi Shingu\",\n          \"Fred Shiosaki\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Bio\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 988,\n        \"samples\": [\n          \"Nisei female Born March in Portland Oregon Grew up in Independence Oregon where parents ran a farm Married and was pregnant with first child before World War II During the war was removed to the Portland Assembly Center Oregon and the Minidoka concentration camp Idaho After leaving camp returned to the Gresham Oregon area\",\n          \"Sansei male Born December in San Jose California Removed as an infant with parents to the Heart Mountain concentration camp Wyoming After leaving camp family resettled in Illinois and Gerald eventually attended the University of Illinois and then moved to the East Coast Became involved in civil rights organizations\",\n          \"Sansei male Born June in Los Angeles California Grew up in Little Tokyo and the Boyle Heights area During World War II was removed to the Poston concentration camp Arizona Became ordained as a Baptist minister while incarcerated ministering to fellow camp inmates and leading ecumenical worship services in camp Left Poston to attend Bethel Theological Seminary in St Paul Minnesota Following the war resettled in Los Angeles and established the Japanese Baptist Church later renamed to Evergreen Baptist Church Appointed the first director of Japanese Evangelical Missionary Society Spent eight years as pastor of the Makiki Church in Honolulu Returned to the mainland and earned his doctorate degree D Rel from the School of Theology Claremont California authoring a thesis on Japanese American identity ethnic pluralism and Christianity Spent fifteen years as Pastor as Japanese Baptist Church in Seattle Washington Taught at the American Baptist Seminary of the West Berkeley California and served as Director of the Council for Pacific Asian Theology Oakland California Presently MinisteratLargeNorthern California Japanese American Church Federation\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Interview Title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 961,\n        \"samples\": [\n          \"Richard Konda Interview ddrjamsj\",\n          \"Dorothy Kuwaye Interview ddrmanz\",\n          \"Michiko Kornhauser Interview ddrone\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Date & Location\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 400,\n        \"samples\": [\n          \"Los Angeles California August Los Angeles California August Los Angeles California\",\n          \"April Los Angeles California\",\n          \"April San Francisco California\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Densho ID\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 36,\n        \"samples\": [\n          \"ddrdensho ddrsjacl\",\n          \"ddrphljacl\",\n          \"ddrdensho ddrdensho ddrdensho ddrdensho ddrdensho\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (3) Remove stopwords by using the stopwords list.\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "input_file = \"/content/Step_2_List_of_narrators.csv\"\n",
        "output_file = \"/content/Step_3_List_of_narrators.csv\"\n",
        "stop_words = set(stopwords.words('english'))\n",
        "def remove_stopwords(text):\n",
        "    if isinstance(text, str):\n",
        "        words = text.split()\n",
        "        filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "        return ' '.join(filtered_words)\n",
        "    return text\n",
        "df = pd.read_csv(input_file, dtype=str)\n",
        "df = df.select_dtypes(include=['object']).applymap(remove_stopwords)\n",
        "df.to_csv(output_file, index=False)\n",
        "df_cleaned = pd.read_csv(output_file)\n",
        "df_cleaned.head()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        },
        "collapsed": true,
        "id": "U9i74DeifRFB",
        "outputId": "d1135a19-d904-4f96-904a-3557f8132d47"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "<ipython-input-12-43fe60551573>:17: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  df = df.select_dtypes(include=['object']).applymap(remove_stopwords)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           Narrator Name                                                Bio  \\\n",
              "0           Kay Aiko Abe  Nisei female Born May Selleck Washington Spent...   \n",
              "1                Art Abe  Nisei male Born June Seattle Washington Grew a...   \n",
              "2  Sharon Tanagi Aburano  Nisei female Born October Seattle Washington F...   \n",
              "3        Toshiko Aiboshi  Nisei female Born July Boyle Heights Californi...   \n",
              "4      Douglas L. Aihara  Sansei male Born March Torrance California Gre...   \n",
              "\n",
              "                                     Interview Title  \\\n",
              "0                   Kay Aiko Abe Interview ddrdensho   \n",
              "1                        Art Abe Interview ddrdensho   \n",
              "2  Sharon Tanagi Aburano Interview II ddrdensho S...   \n",
              "3                  Toshiko Aiboshi Interview ddrmanz   \n",
              "4               Douglas L Aihara Interview ddrdensho   \n",
              "\n",
              "                                     Date & Location            Densho ID  \n",
              "0                        December Seattle Washington            ddrdensho  \n",
              "1                         January Seattle Washington            ddrdensho  \n",
              "2  April Seattle Washington March Seattle Washington  ddrdensho ddrdensho  \n",
              "3                     January Culver City California              ddrmanz  \n",
              "4                         Nov Los Angeles California            ddrdensho  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9f66b1e6-8f3d-455c-9183-0f8853b0e91a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Narrator Name</th>\n",
              "      <th>Bio</th>\n",
              "      <th>Interview Title</th>\n",
              "      <th>Date &amp; Location</th>\n",
              "      <th>Densho ID</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Kay Aiko Abe</td>\n",
              "      <td>Nisei female Born May Selleck Washington Spent...</td>\n",
              "      <td>Kay Aiko Abe Interview ddrdensho</td>\n",
              "      <td>December Seattle Washington</td>\n",
              "      <td>ddrdensho</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Art Abe</td>\n",
              "      <td>Nisei male Born June Seattle Washington Grew a...</td>\n",
              "      <td>Art Abe Interview ddrdensho</td>\n",
              "      <td>January Seattle Washington</td>\n",
              "      <td>ddrdensho</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Sharon Tanagi Aburano</td>\n",
              "      <td>Nisei female Born October Seattle Washington F...</td>\n",
              "      <td>Sharon Tanagi Aburano Interview II ddrdensho S...</td>\n",
              "      <td>April Seattle Washington March Seattle Washington</td>\n",
              "      <td>ddrdensho ddrdensho</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Toshiko Aiboshi</td>\n",
              "      <td>Nisei female Born July Boyle Heights Californi...</td>\n",
              "      <td>Toshiko Aiboshi Interview ddrmanz</td>\n",
              "      <td>January Culver City California</td>\n",
              "      <td>ddrmanz</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Douglas L. Aihara</td>\n",
              "      <td>Sansei male Born March Torrance California Gre...</td>\n",
              "      <td>Douglas L Aihara Interview ddrdensho</td>\n",
              "      <td>Nov Los Angeles California</td>\n",
              "      <td>ddrdensho</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9f66b1e6-8f3d-455c-9183-0f8853b0e91a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-9f66b1e6-8f3d-455c-9183-0f8853b0e91a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-9f66b1e6-8f3d-455c-9183-0f8853b0e91a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-bdbb21fd-038f-4a3d-883e-4309af152101\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-bdbb21fd-038f-4a3d-883e-4309af152101')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-bdbb21fd-038f-4a3d-883e-4309af152101 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_cleaned",
              "summary": "{\n  \"name\": \"df_cleaned\",\n  \"rows\": 1002,\n  \"fields\": [\n    {\n      \"column\": \"Narrator Name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1000,\n        \"samples\": [\n          \"Masahiro Nakajo\",\n          \"Fred Tadashi Shingu\",\n          \"Fred Shiosaki\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Bio\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 988,\n        \"samples\": [\n          \"Nisei female Born March Portland Oregon Grew Independence Oregon parents ran farm Married pregnant first child World War II war removed Portland Assembly Center Oregon Minidoka concentration camp Idaho leaving camp returned Gresham Oregon area\",\n          \"Sansei male Born December San Jose California Removed infant parents Heart Mountain concentration camp Wyoming leaving camp family resettled Illinois Gerald eventually attended University Illinois moved East Coast Became involved civil rights organizations\",\n          \"Sansei male Born June Los Angeles California Grew Little Tokyo Boyle Heights area World War II removed Poston concentration camp Arizona Became ordained Baptist minister incarcerated ministering fellow camp inmates leading ecumenical worship services camp Left Poston attend Bethel Theological Seminary St Paul Minnesota Following war resettled Los Angeles established Japanese Baptist Church later renamed Evergreen Baptist Church Appointed first director Japanese Evangelical Missionary Society Spent eight years pastor Makiki Church Honolulu Returned mainland earned doctorate degree Rel School Theology Claremont California authoring thesis Japanese American identity ethnic pluralism Christianity Spent fifteen years Pastor Japanese Baptist Church Seattle Washington Taught American Baptist Seminary West Berkeley California served Director Council Pacific Asian Theology Oakland California Presently MinisteratLargeNorthern California Japanese American Church Federation\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Interview Title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 961,\n        \"samples\": [\n          \"Richard Konda Interview ddrjamsj\",\n          \"Dorothy Kuwaye Interview ddrmanz\",\n          \"Michiko Kornhauser Interview ddrone\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Date & Location\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 400,\n        \"samples\": [\n          \"Los Angeles California August Los Angeles California August Los Angeles California\",\n          \"April Los Angeles California\",\n          \"April San Francisco California\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Densho ID\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 36,\n        \"samples\": [\n          \"ddrdensho ddrsjacl\",\n          \"ddrphljacl\",\n          \"ddrdensho ddrdensho ddrdensho ddrdensho ddrdensho\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#(4) Lowercase all texts\n",
        "import pandas as pd\n",
        "\n",
        "input_file = \"/content/Step_3_List_of_narrators.csv\"\n",
        "output_file = \"/content/Step_4_List_of_narrators.csv\"\n",
        "df = pd.read_csv(input_file, dtype=str).apply(lambda x: x.str.lower() if x.dtype == \"object\" else x)\n",
        "df.to_csv(output_file, index=False)\n",
        "df_cleaned = pd.read_csv(output_file)\n",
        "df_cleaned.head()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "collapsed": true,
        "id": "ANJ5oy1hhO_q",
        "outputId": "40bbdbb1-f1d8-47a3-d81e-d6ab6e47d6d2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           Narrator Name                                                Bio  \\\n",
              "0           kay aiko abe  nisei female born may selleck washington spent...   \n",
              "1                art abe  nisei male born june seattle washington grew a...   \n",
              "2  sharon tanagi aburano  nisei female born october seattle washington f...   \n",
              "3        toshiko aiboshi  nisei female born july boyle heights californi...   \n",
              "4      douglas l. aihara  sansei male born march torrance california gre...   \n",
              "\n",
              "                                     Interview Title  \\\n",
              "0                   kay aiko abe interview ddrdensho   \n",
              "1                        art abe interview ddrdensho   \n",
              "2  sharon tanagi aburano interview ii ddrdensho s...   \n",
              "3                  toshiko aiboshi interview ddrmanz   \n",
              "4               douglas l aihara interview ddrdensho   \n",
              "\n",
              "                                     Date & Location            Densho ID  \n",
              "0                        december seattle washington            ddrdensho  \n",
              "1                         january seattle washington            ddrdensho  \n",
              "2  april seattle washington march seattle washington  ddrdensho ddrdensho  \n",
              "3                     january culver city california              ddrmanz  \n",
              "4                         nov los angeles california            ddrdensho  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b13731eb-4bdb-484e-a4a3-d9eca8524a8a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Narrator Name</th>\n",
              "      <th>Bio</th>\n",
              "      <th>Interview Title</th>\n",
              "      <th>Date &amp; Location</th>\n",
              "      <th>Densho ID</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>kay aiko abe</td>\n",
              "      <td>nisei female born may selleck washington spent...</td>\n",
              "      <td>kay aiko abe interview ddrdensho</td>\n",
              "      <td>december seattle washington</td>\n",
              "      <td>ddrdensho</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>art abe</td>\n",
              "      <td>nisei male born june seattle washington grew a...</td>\n",
              "      <td>art abe interview ddrdensho</td>\n",
              "      <td>january seattle washington</td>\n",
              "      <td>ddrdensho</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>sharon tanagi aburano</td>\n",
              "      <td>nisei female born october seattle washington f...</td>\n",
              "      <td>sharon tanagi aburano interview ii ddrdensho s...</td>\n",
              "      <td>april seattle washington march seattle washington</td>\n",
              "      <td>ddrdensho ddrdensho</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>toshiko aiboshi</td>\n",
              "      <td>nisei female born july boyle heights californi...</td>\n",
              "      <td>toshiko aiboshi interview ddrmanz</td>\n",
              "      <td>january culver city california</td>\n",
              "      <td>ddrmanz</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>douglas l. aihara</td>\n",
              "      <td>sansei male born march torrance california gre...</td>\n",
              "      <td>douglas l aihara interview ddrdensho</td>\n",
              "      <td>nov los angeles california</td>\n",
              "      <td>ddrdensho</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b13731eb-4bdb-484e-a4a3-d9eca8524a8a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b13731eb-4bdb-484e-a4a3-d9eca8524a8a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b13731eb-4bdb-484e-a4a3-d9eca8524a8a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-6fbef52c-ca62-4b71-aa5d-f2573709391b\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-6fbef52c-ca62-4b71-aa5d-f2573709391b')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-6fbef52c-ca62-4b71-aa5d-f2573709391b button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_cleaned",
              "summary": "{\n  \"name\": \"df_cleaned\",\n  \"rows\": 1002,\n  \"fields\": [\n    {\n      \"column\": \"Narrator Name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1000,\n        \"samples\": [\n          \"masahiro nakajo\",\n          \"fred tadashi shingu\",\n          \"fred shiosaki\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Bio\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 987,\n        \"samples\": [\n          \"nisei male born november mountain view hawaii grew hawaii parents ran sugar cane plantation working hauling lumber japan attacked pearl harbor december dismissed work like japanese americans required clean aftermath bombing volunteered army although five feet tall able enlist account paperwork mistake served italy received bronze star saving life fellow soldier discharge married eventually settled seattle washington\",\n          \"nisei female born march raised seattle washington bombing pearl harbor removed puyallup assembly center washington minidoka concentration camp idaho left minidoka attend school st paul minnesota married lived japan sixteen months returning united states raising family\",\n          \"sansei male born june los angeles california grew little tokyo boyle heights area world war ii removed poston concentration camp arizona became ordained baptist minister incarcerated ministering fellow camp inmates leading ecumenical worship services camp left poston attend bethel theological seminary st paul minnesota following war resettled los angeles established japanese baptist church later renamed evergreen baptist church appointed first director japanese evangelical missionary society spent eight years pastor makiki church honolulu returned mainland earned doctorate degree rel school theology claremont california authoring thesis japanese american identity ethnic pluralism christianity spent fifteen years pastor japanese baptist church seattle washington taught american baptist seminary west berkeley california served director council pacific asian theology oakland california presently ministeratlargenorthern california japanese american church federation\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Interview Title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 961,\n        \"samples\": [\n          \"richard konda interview ddrjamsj\",\n          \"dorothy kuwaye interview ddrmanz\",\n          \"michiko kornhauser interview ddrone\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Date & Location\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 400,\n        \"samples\": [\n          \"los angeles california august los angeles california august los angeles california\",\n          \"april los angeles california\",\n          \"april san francisco california\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Densho ID\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 36,\n        \"samples\": [\n          \"ddrdensho ddrsjacl\",\n          \"ddrphljacl\",\n          \"ddrdensho ddrdensho ddrdensho ddrdensho ddrdensho\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#(5) Stemming\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "nltk.download('punkt', force=True)\n",
        "\n",
        "input_file = \"/content/Step_4_List_of_narrators.csv\"\n",
        "output_file = \"/content/Step_5_List_of_narrators.csv\"\n",
        "st = PorterStemmer()\n",
        "df = pd.read_csv(input_file, dtype=str).fillna('')\n",
        "df['Bio'] = df['Bio'].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))\n",
        "df.to_csv(output_file, index=False)\n",
        "df_cleaned = pd.read_csv(output_file)\n",
        "df_cleaned.head()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327
        },
        "collapsed": true,
        "id": "akOI0CX3kjBk",
        "outputId": "83a1a47c-1ca4-415b-a0b2-6734be92079a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           Narrator Name                                                Bio  \\\n",
              "0           kay aiko abe  nisei femal born may selleck washington spent ...   \n",
              "1                art abe  nisei male born june seattl washington grew ar...   \n",
              "2  sharon tanagi aburano  nisei femal born octob seattl washington famil...   \n",
              "3        toshiko aiboshi  nisei femal born juli boyl height california e...   \n",
              "4      douglas l. aihara  sansei male born march torranc california grew...   \n",
              "\n",
              "                                     Interview Title  \\\n",
              "0                   kay aiko abe interview ddrdensho   \n",
              "1                        art abe interview ddrdensho   \n",
              "2  sharon tanagi aburano interview ii ddrdensho s...   \n",
              "3                  toshiko aiboshi interview ddrmanz   \n",
              "4               douglas l aihara interview ddrdensho   \n",
              "\n",
              "                                     Date & Location            Densho ID  \n",
              "0                        december seattle washington            ddrdensho  \n",
              "1                         january seattle washington            ddrdensho  \n",
              "2  april seattle washington march seattle washington  ddrdensho ddrdensho  \n",
              "3                     january culver city california              ddrmanz  \n",
              "4                         nov los angeles california            ddrdensho  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8e64ace3-273f-4d0f-8889-28a87ac99730\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Narrator Name</th>\n",
              "      <th>Bio</th>\n",
              "      <th>Interview Title</th>\n",
              "      <th>Date &amp; Location</th>\n",
              "      <th>Densho ID</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>kay aiko abe</td>\n",
              "      <td>nisei femal born may selleck washington spent ...</td>\n",
              "      <td>kay aiko abe interview ddrdensho</td>\n",
              "      <td>december seattle washington</td>\n",
              "      <td>ddrdensho</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>art abe</td>\n",
              "      <td>nisei male born june seattl washington grew ar...</td>\n",
              "      <td>art abe interview ddrdensho</td>\n",
              "      <td>january seattle washington</td>\n",
              "      <td>ddrdensho</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>sharon tanagi aburano</td>\n",
              "      <td>nisei femal born octob seattl washington famil...</td>\n",
              "      <td>sharon tanagi aburano interview ii ddrdensho s...</td>\n",
              "      <td>april seattle washington march seattle washington</td>\n",
              "      <td>ddrdensho ddrdensho</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>toshiko aiboshi</td>\n",
              "      <td>nisei femal born juli boyl height california e...</td>\n",
              "      <td>toshiko aiboshi interview ddrmanz</td>\n",
              "      <td>january culver city california</td>\n",
              "      <td>ddrmanz</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>douglas l. aihara</td>\n",
              "      <td>sansei male born march torranc california grew...</td>\n",
              "      <td>douglas l aihara interview ddrdensho</td>\n",
              "      <td>nov los angeles california</td>\n",
              "      <td>ddrdensho</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8e64ace3-273f-4d0f-8889-28a87ac99730')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-8e64ace3-273f-4d0f-8889-28a87ac99730 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-8e64ace3-273f-4d0f-8889-28a87ac99730');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-512d5200-45f0-46c8-82f9-4f50320b8dc1\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-512d5200-45f0-46c8-82f9-4f50320b8dc1')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-512d5200-45f0-46c8-82f9-4f50320b8dc1 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_cleaned",
              "summary": "{\n  \"name\": \"df_cleaned\",\n  \"rows\": 1002,\n  \"fields\": [\n    {\n      \"column\": \"Narrator Name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1000,\n        \"samples\": [\n          \"masahiro nakajo\",\n          \"fred tadashi shingu\",\n          \"fred shiosaki\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Bio\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 987,\n        \"samples\": [\n          \"nisei male born novemb mountain view hawaii grew hawaii parent ran sugar cane plantat work haul lumber japan attack pearl harbor decemb dismiss work like japanes american requir clean aftermath bomb volunt armi although five feet tall abl enlist account paperwork mistak serv itali receiv bronz star save life fellow soldier discharg marri eventu settl seattl washington\",\n          \"nisei femal born march rais seattl washington bomb pearl harbor remov puyallup assembl center washington minidoka concentr camp idaho left minidoka attend school st paul minnesota marri live japan sixteen month return unit state rais famili\",\n          \"sansei male born june lo angel california grew littl tokyo boyl height area world war ii remov poston concentr camp arizona becam ordain baptist minist incarcer minist fellow camp inmat lead ecumen worship servic camp left poston attend bethel theolog seminari st paul minnesota follow war resettl lo angel establish japanes baptist church later renam evergreen baptist church appoint first director japanes evangel missionari societi spent eight year pastor makiki church honolulu return mainland earn doctor degre rel school theolog claremont california author thesi japanes american ident ethnic plural christian spent fifteen year pastor japanes baptist church seattl washington taught american baptist seminari west berkeley california serv director council pacif asian theolog oakland california present ministeratlargenorthern california japanes american church feder\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Interview Title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 961,\n        \"samples\": [\n          \"richard konda interview ddrjamsj\",\n          \"dorothy kuwaye interview ddrmanz\",\n          \"michiko kornhauser interview ddrone\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Date & Location\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 400,\n        \"samples\": [\n          \"los angeles california august los angeles california august los angeles california\",\n          \"april los angeles california\",\n          \"april san francisco california\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Densho ID\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 36,\n        \"samples\": [\n          \"ddrdensho ddrsjacl\",\n          \"ddrphljacl\",\n          \"ddrdensho ddrdensho ddrdensho ddrdensho ddrdensho\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspellchecker\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "wcCMoJGCfkVd",
        "outputId": "669689ec-bac6-4b75-a979-4a4602886c72"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspellchecker\n",
            "  Downloading pyspellchecker-0.8.2-py3-none-any.whl.metadata (9.4 kB)\n",
            "Downloading pyspellchecker-0.8.2-py3-none-any.whl (7.1 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/7.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/7.1 MB\u001b[0m \u001b[31m121.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m124.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m71.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyspellchecker\n",
            "Successfully installed pyspellchecker-0.8.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (6) Lemmatization.\n",
        "import pandas as pd\n",
        "import spacy\n",
        "from spellchecker import SpellChecker\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "spell = SpellChecker()\n",
        "\n",
        "file_path = \"/content/Step_5_List_of_narrators.csv\"\n",
        "output_file = \"/content/Step_6_List_of_narrators.csv\"\n",
        "\n",
        "df = pd.read_csv(file_path, dtype=str).fillna('')\n",
        "\n",
        "def correct_and_lemmatize(text):\n",
        "    if not isinstance(text, str) or text.strip() == \"\":\n",
        "        return \"\"\n",
        "\n",
        "    words = text.split()\n",
        "    corrected_words = [spell.correction(word) if spell.correction(word) else word for word in words]\n",
        "    corrected_text = \" \".join(corrected_words)\n",
        "\n",
        "    doc = nlp(corrected_text)\n",
        "    return \" \".join([token.lemma_ for token in doc])\n",
        "\n",
        "df['Bio'] = df['Bio'].apply(correct_and_lemmatize)\n",
        "\n",
        "df.to_csv(output_file, index=False)\n",
        "print(f\"Data saved to {output_file}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "oUYvTFid8ObW",
        "outputId": "9d67c652-4227-486a-fa1d-b13eb1fdcb86"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data saved to /content/Step_6_List_of_narrators.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1F_PZdH9Sh49"
      },
      "source": [
        "# Question 3 (15 points)\n",
        "\n",
        "Write a python program to **conduct syntax and structure analysis of the clean text** you just saved above. The syntax and structure analysis includes:\n",
        "\n",
        "(1) **Parts of Speech (POS) Tagging:** Tag Parts of Speech of each word in the text, and calculate the total number of N(oun), V(erb), Adj(ective), Adv(erb), respectively.\n",
        "\n",
        "(2) **Constituency Parsing and Dependency Parsing:** print out the constituency parsing trees and dependency parsing trees of all the sentences. Using one sentence as an example to explain your understanding about the constituency parsing tree and dependency parsing tree.\n",
        "\n",
        "(3) **Named Entity Recognition:** Extract all the entities such as person names, organizations, locations, product names, and date from the clean texts, calculate the count of each entity."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#(1) Parts of Speech (POS) Tagging:\n",
        "import pandas as pd\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "file_path = \"/content/Step_6_List_of_narrators.csv\"\n",
        "\n",
        "df = pd.read_csv(file_path, dtype=str).fillna('')\n",
        "\n",
        "def pos_analysis(text):\n",
        "    if not isinstance(text, str) or text.strip() == \"\":\n",
        "        return {'N': 0, 'V': 0, 'Adj': 0, 'Adv': 0}\n",
        "    doc = nlp(text)\n",
        "    pos_counts = {'N': 0, 'V': 0, 'Adj': 0, 'Adv': 0}\n",
        "\n",
        "    for token in doc:\n",
        "        if token.pos_ == \"NOUN\":\n",
        "            pos_counts['N'] += 1\n",
        "        elif token.pos_ == \"VERB\":\n",
        "            pos_counts['V'] += 1\n",
        "        elif token.pos_ == \"ADJ\":\n",
        "            pos_counts['Adj'] += 1\n",
        "        elif token.pos_ == \"ADV\":\n",
        "            pos_counts['Adv'] += 1\n",
        "    return pos_counts\n",
        "df['POS_Counts'] = df['Bio'].apply(pos_analysis)\n",
        "pos_df = pd.DataFrame(df['POS_Counts'].tolist())\n",
        "\n",
        "def save_results(dataframe, filename):\n",
        "    output_path = f\"/content/{filename}\"\n",
        "    dataframe.to_csv(output_path, index=False)\n",
        "    print(f\"Results saved to {output_path}\")\n",
        "save_results(pos_df, \"Pos_analysis_results.csv\")\n",
        "total_counts = pd.DataFrame([pos_df.sum()])\n",
        "save_results(total_counts, \"Total_pos_counts.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "j8rpobDnxHAl",
        "outputId": "8cbf2137-d458-455c-d3a3-8d18ade9d831"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results saved to /content/Pos_analysis_results.csv\n",
            "Results saved to /content/Total_pos_counts.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Constituency Parsing and Dependency Parsing\n",
        "import pandas as pd\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "file_path = \"/content/Step_6_List_of_narrators.csv\"\n",
        "df = pd.read_csv(file_path, dtype=str).fillna('')\n",
        "\n",
        "if \"Bio\" in df.columns and not df.empty:\n",
        "    bio_text = df.loc[0, 'Bio']\n",
        "else:\n",
        "    raise ValueError(\"Error: 'Bio' column not found or dataset is empty.\")\n",
        "\n",
        "def parse_dependencies(text):\n",
        "    doc = nlp(text)\n",
        "    return [(token.text, token.dep_, token.head.text) for token in doc]\n",
        "\n",
        "print(\"\\nDependency Parsing Tree for First Narator:\")\n",
        "dependency_tree = parse_dependencies(bio_text)\n",
        "\n",
        "for word, relation, head in dependency_tree:\n",
        "    print(f\"{word} - {relation}- {head}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "XI3CVE9ibLdI",
        "outputId": "3b407bb8-d816-4eb7-8ab6-2e7c7aa64430"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dependency Parsing Tree for First Narator:\n",
            "nisei - amod- bear\n",
            "female - amod- bear\n",
            "bear - nsubj- select\n",
            "may - aux- select\n",
            "select - ccomp- spend\n",
            "washington - dobj- select\n",
            "spend - ROOT- spend\n",
            "much - amod- beaverton\n",
            "childhood - compound- beaverton\n",
            "beaverton - dobj- spend\n",
            "oregon - compound- father\n",
            "father - appos- beaverton\n",
            "own - xcomp- spend\n",
            "farm - compound- influence\n",
            "influence - dobj- own\n",
            "early - amod- age\n",
            "age - compound- parent\n",
            "parent - nsubj- cover\n",
            "cover - conj- spend\n",
            "christian - amod- war\n",
            "world - compound- war\n",
            "war - dobj- cover\n",
            "I - nsubj- remove\n",
            "remove - relcl- war\n",
            "portland - nmod- center\n",
            "assemble - compound- center\n",
            "center - compound- oregon\n",
            "oregon - compound- work\n",
            "minidoka - compound- work\n",
            "concent - compound- camp\n",
            "camp - compound- work\n",
            "idaho - compound- war\n",
            "war - compound- work\n",
            "work - nsubj- establish\n",
            "establish - ccomp- remove\n",
            "success - compound- feed\n",
            "volant - amod- program\n",
            "program - compound- feed\n",
            "feed - dobj- establish\n",
            "homeless - compound- seattle\n",
            "seattle - appos- feed\n",
            "washington - npadvmod- establish\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#(3) Name Entity Recognition:\n",
        "import pandas as pd\n",
        "import spacy\n",
        "from collections import Counter\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "file_path = \"/content/Step_6_List_of_narrators.csv\"\n",
        "df = pd.read_csv(file_path, dtype=str).fillna('')\n",
        "\n",
        "columns_to_process = [\"Narrator Name\", \"Bio\", \"Interview Title\", \"Date & Location\"]\n",
        "entity_counts = Counter()\n",
        "unique_entities = set()\n",
        "\n",
        "for column in columns_to_process:\n",
        "    if column in df.columns:\n",
        "        for text in df[column].dropna():\n",
        "            doc = nlp(text)\n",
        "            for ent in doc.ents:\n",
        "                unique_entities.add((ent.text, ent.label_))\n",
        "\n",
        "for _, label in unique_entities:\n",
        "    entity_counts[label] += 1\n",
        "\n",
        "print(\"\\nName Entity Counts:\")\n",
        "for entity, count in entity_counts.items():\n",
        "    print(f\"{entity}: {count}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "DX8BtDy8eccc",
        "outputId": "764af113-3c35-4172-ab93-d966565ad2cb"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Name Entity Counts:\n",
            "PERSON: 1381\n",
            "ORG: 614\n",
            "GPE: 210\n",
            "PRODUCT: 10\n",
            "LOC: 27\n",
            "NORP: 55\n",
            "DATE: 119\n",
            "CARDINAL: 31\n",
            "QUANTITY: 3\n",
            "FAC: 26\n",
            "LANGUAGE: 3\n",
            "EVENT: 14\n",
            "MONEY: 2\n",
            "LAW: 2\n",
            "ORDINAL: 7\n",
            "TIME: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Following Questions must answer using AI assitance**"
      ],
      "metadata": {
        "id": "EcVqy1yj3wja"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 4 (20 points)."
      ],
      "metadata": {
        "id": "kEdcyHX8VaDB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4. (PART-1)\n",
        "Web scraping data from the GitHub Marketplace to gather details about popular actions. Using Python, the process begins by sending HTTP requests to multiple pages of the marketplace (1000 products), handling pagination through dynamic page numbers. The key details extracted include the product name, a short description, and the URL.\n",
        "\n",
        " The extracted data is stored in a structured CSV format with columns for product name, description, URL, and page number. A time delay is introduced between requests to avoid server overload. ChatGPT can assist by helping with the parsing of HTML, error handling, and generating reports based on the data collected.\n",
        "\n",
        " The goal is to complete the scraping within a specified time limit, ensuring that the process is efficient and adheres to GitHub’s usage guidelines.\n",
        "\n",
        "(PART -2)\n",
        "\n",
        "1.   **Preprocess Data**: Clean the text by tokenizing, removing stopwords, and converting to lowercase.\n",
        "\n",
        "2. Perform **Data Quality** operations.\n",
        "\n",
        "\n",
        "Preprocessing:\n",
        "Preprocessing involves cleaning the text by removing noise such as special characters, HTML tags, and unnecessary whitespace. It also includes tasks like tokenization, stopword removal, and lemmatization to standardize the text for analysis.\n",
        "\n",
        "Data Quality:\n",
        "Data quality checks ensure completeness, consistency, and accuracy by verifying that all required columns are filled and formatted correctly. Additionally, it involves identifying and removing duplicates, handling missing values, and ensuring the data reflects the true content accurately.\n"
      ],
      "metadata": {
        "id": "1Ung5_YW3C6y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Github MarketPlace page:\n",
        "https://github.com/marketplace?type=actions"
      ],
      "metadata": {
        "id": "CTOfUpatronW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prompt:Give me a code using python to scrape data such as Product name, Link, description, concurrent page number and format it in a csv file"
      ],
      "metadata": {
        "id": "eurr6BIqUMa4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "from urllib.request import Request, urlopen\n",
        "import csv\n",
        "import requests\n",
        "import time\n",
        "import random\n",
        "\n",
        "# Function for exponential backoff\n",
        "def sleep_with_backoff(attempt):\n",
        "    sleep_time = min(2 ** attempt + random.uniform(0, 1), 10)  # Exponential backoff up to 10 sec\n",
        "    print(f\"Retrying in {sleep_time:.2f} seconds...\")\n",
        "    time.sleep(sleep_time)\n",
        "\n",
        "# Initialize session\n",
        "session = requests.Session()\n",
        "session.headers.update({'User-Agent': 'Mozilla/5.0'})\n",
        "\n",
        "# Function to extract action links from a specific page of the GitHub Marketplace\n",
        "def extract_action_links(page_number):\n",
        "    main_url = f\"https://github.com/marketplace?type=actions&page={page_number}\"\n",
        "    action_links = []\n",
        "    print(f\"Processing Page {page_number}\")\n",
        "\n",
        "    for attempt in range(5):  # Retry up to 5 times\n",
        "        try:\n",
        "            req = Request(main_url, headers={'User-Agent': 'Mozilla/5.0'})\n",
        "            page = urlopen(req)\n",
        "            soup = BeautifulSoup(page, 'html.parser')\n",
        "\n",
        "            for action in soup.find_all(\"div\", attrs={'data-testid': 'non-featured-item'}):\n",
        "                action_link_tag = action.find(\"a\", href=True)\n",
        "                action_name = action_link_tag.text.strip() if action_link_tag else \"N/A\"\n",
        "                action_link = f\"https://github.com{action_link_tag['href']}\" if action_link_tag else \"N/A\"\n",
        "\n",
        "                action_links.append({\n",
        "                    'Page Number': page_number,\n",
        "                    'Action Name': action_name,\n",
        "                    'Link': action_link,\n",
        "                    'Description': \"N/A\"\n",
        "                })\n",
        "\n",
        "            print(f\"✅ Extracted {len(action_links)} action links.\")\n",
        "            break  # Exit retry loop on success\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Error occurred while processing page {page_number}: {e}\")\n",
        "            sleep_with_backoff(attempt)\n",
        "\n",
        "    return action_links\n",
        "\n",
        "# Function to extract descriptions from each action's page\n",
        "def extract_description(action_links):\n",
        "    for action in action_links:\n",
        "        url = action['Link']\n",
        "\n",
        "        for attempt in range(5):  # Retry up to 5 times\n",
        "            try:\n",
        "                response = session.get(url, timeout=10)\n",
        "\n",
        "                if response.status_code == 200:\n",
        "                    soup = BeautifulSoup(response.content, 'lxml')\n",
        "                    div_tag = soup.find('div', attrs={'data-testid': 'about'})\n",
        "\n",
        "                    if div_tag:\n",
        "                        span_tag = div_tag.find('span')\n",
        "                        if span_tag:\n",
        "                            action['Description'] = span_tag.get_text().strip()\n",
        "\n",
        "                break  # Exit retry loop on success\n",
        "            except requests.exceptions.RequestException as e:\n",
        "                print(f\"Error fetching {url}: {e}\")\n",
        "                sleep_with_backoff(attempt)\n",
        "\n",
        "    return action_links\n",
        "\n",
        "# Function to save data to a CSV file\n",
        "def save_to_csv(data, filename):\n",
        "    with open(filename, mode='w', newline='', encoding='utf-8') as file:\n",
        "        writer = csv.DictWriter(file, fieldnames=['Page Number', 'Action Name', 'Link', 'Description'])\n",
        "        writer.writeheader()\n",
        "        writer.writerows(data)\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    all_action_links = []\n",
        "\n",
        "    # Step 1: Extract action links from pages 1 to 60\n",
        "    for page_number in range(1, 60):\n",
        "        action_links = extract_action_links(page_number)\n",
        "        all_action_links.extend(action_links)\n",
        "\n",
        "        # Adding a delay between requests to avoid hitting rate limits\n",
        "        time.sleep(5)  # Increased to 5 seconds\n",
        "\n",
        "    # Step 2: Extract descriptions for each action\n",
        "    updated_action_links = extract_description(all_action_links)\n",
        "\n",
        "    # Step 3: Save final data to a CSV file\n",
        "    save_to_csv(updated_action_links, \"github_actions_final_data.csv\")\n",
        "\n",
        "    print(\"✅ Data extraction completed and saved to 'github_actions_final_data.csv'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "BPmnG44wA-gY",
        "outputId": "37c8610c-96b4-447d-e958-daf9c85c58b1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing Page 1\n",
            "✅ Extracted 20 action links.\n",
            "Processing Page 2\n",
            "✅ Extracted 20 action links.\n",
            "Processing Page 3\n",
            "✅ Extracted 20 action links.\n",
            "Processing Page 4\n",
            "✅ Extracted 20 action links.\n",
            "Processing Page 5\n",
            "✅ Extracted 20 action links.\n",
            "Processing Page 6\n",
            "✅ Extracted 20 action links.\n",
            "Processing Page 7\n",
            "✅ Extracted 20 action links.\n",
            "Processing Page 8\n",
            "✅ Extracted 0 action links.\n",
            "Processing Page 9\n",
            "✅ Extracted 20 action links.\n",
            "Processing Page 10\n",
            "✅ Extracted 0 action links.\n",
            "Processing Page 11\n",
            "✅ Extracted 20 action links.\n",
            "Processing Page 12\n",
            "✅ Extracted 20 action links.\n",
            "Processing Page 13\n",
            "✅ Extracted 20 action links.\n",
            "Processing Page 14\n",
            "✅ Extracted 20 action links.\n",
            "Processing Page 15\n",
            "✅ Extracted 20 action links.\n",
            "Processing Page 16\n",
            "✅ Extracted 0 action links.\n",
            "Processing Page 17\n",
            "✅ Extracted 20 action links.\n",
            "Processing Page 18\n",
            "✅ Extracted 20 action links.\n",
            "Processing Page 19\n",
            "✅ Extracted 20 action links.\n",
            "Processing Page 20\n",
            "✅ Extracted 20 action links.\n",
            "Processing Page 21\n",
            "✅ Extracted 20 action links.\n",
            "Processing Page 22\n",
            "✅ Extracted 20 action links.\n",
            "Processing Page 23\n",
            "✅ Extracted 20 action links.\n",
            "Processing Page 24\n",
            "✅ Extracted 20 action links.\n",
            "Processing Page 25\n",
            "✅ Extracted 20 action links.\n",
            "Processing Page 26\n",
            "✅ Extracted 20 action links.\n",
            "Processing Page 27\n",
            "✅ Extracted 20 action links.\n",
            "Processing Page 28\n",
            "✅ Extracted 20 action links.\n",
            "Processing Page 29\n",
            "✅ Extracted 20 action links.\n",
            "Processing Page 30\n",
            "✅ Extracted 20 action links.\n",
            "Processing Page 31\n",
            "✅ Extracted 20 action links.\n",
            "Processing Page 32\n",
            "✅ Extracted 20 action links.\n",
            "Processing Page 33\n",
            "✅ Extracted 20 action links.\n",
            "Processing Page 34\n",
            "✅ Extracted 20 action links.\n",
            "Processing Page 35\n",
            "✅ Extracted 20 action links.\n",
            "Processing Page 36\n",
            "✅ Extracted 20 action links.\n",
            "Processing Page 37\n",
            "✅ Extracted 20 action links.\n",
            "Processing Page 38\n",
            "✅ Extracted 20 action links.\n",
            "Processing Page 39\n",
            "✅ Extracted 20 action links.\n",
            "Processing Page 40\n",
            "✅ Extracted 20 action links.\n",
            "Processing Page 41\n",
            "✅ Extracted 20 action links.\n",
            "Processing Page 42\n",
            "✅ Extracted 20 action links.\n",
            "Processing Page 43\n",
            "✅ Extracted 20 action links.\n",
            "Processing Page 44\n",
            "✅ Extracted 20 action links.\n",
            "Processing Page 45\n",
            "✅ Extracted 20 action links.\n",
            "Processing Page 46\n",
            "✅ Extracted 20 action links.\n",
            "Processing Page 47\n",
            "✅ Extracted 20 action links.\n",
            "Processing Page 48\n",
            "✅ Extracted 20 action links.\n",
            "Processing Page 49\n",
            "✅ Extracted 20 action links.\n",
            "Processing Page 50\n",
            "✅ Extracted 20 action links.\n",
            "Processing Page 51\n",
            "✅ Extracted 20 action links.\n",
            "Processing Page 52\n",
            "✅ Extracted 20 action links.\n",
            "Processing Page 53\n",
            "✅ Extracted 20 action links.\n",
            "Processing Page 54\n",
            "✅ Extracted 20 action links.\n",
            "Processing Page 55\n",
            "✅ Extracted 20 action links.\n",
            "Processing Page 56\n",
            "✅ Extracted 20 action links.\n",
            "Processing Page 57\n",
            "✅ Extracted 20 action links.\n",
            "Processing Page 58\n",
            "✅ Extracted 20 action links.\n",
            "Processing Page 59\n",
            "✅ Extracted 20 action links.\n",
            "✅ Data extraction completed and saved to 'github_actions_final_data.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prompt: Give me the python code to clean the extraced file by utilising the necessary libraries for it"
      ],
      "metadata": {
        "id": "FWraplqpUwVO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "from textblob import TextBlob  # Import TextBlob for spell correction\n",
        "\n",
        "# Download NLTK resources (run this once)\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Load the CSV file (update the path)\n",
        "file_path = '/content/github_actions_final_data.csv'  # Update with actual file path\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Function to preprocess text (cleaning, tokenizing, stopword removal, lemmatization, spell correction)\n",
        "def preprocess_text(text):\n",
        "    if pd.isna(text):\n",
        "        return \"No description available\"\n",
        "\n",
        "    # Remove special characters and HTML tags\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Keep only letters and spaces\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "\n",
        "    # Tokenization\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    # Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "    # Spell Correction using TextBlob (can be slow for large datasets)\n",
        "    corrected_tokens = [str(TextBlob(word).correct()) for word in tokens]\n",
        "\n",
        "    # Rejoin tokens into a string\n",
        "    return ' '.join(corrected_tokens)\n",
        "\n",
        "# Function to perform data quality checks (handle missing values and duplicates)\n",
        "def data_quality_checks(data):\n",
        "    # Remove duplicates\n",
        "    data = data.drop_duplicates()\n",
        "\n",
        "    # Handle missing values (fill with placeholders)\n",
        "    data['Action Name'] = data['Action Name'].fillna('Unknown')\n",
        "    data['Description'] = data['Description'].fillna('No description available')\n",
        "\n",
        "    return data\n",
        "\n",
        "# Step 1: Preprocess the 'Action Name' and 'Description' columns\n",
        "df['Action Name'] = df['Action Name'].apply(preprocess_text)\n",
        "df['Description'] = df['Description'].apply(preprocess_text)\n",
        "\n",
        "# Step 2: Perform Data Quality checks\n",
        "final_data = data_quality_checks(df)\n",
        "\n",
        "# Step 3: Save the processed data to a new CSV file\n",
        "output_file_path = '/content/github_actions_processed_data.csv'  # Update with actual path\n",
        "final_data.to_csv(output_file_path, index=False)\n",
        "\n",
        "# Print the path to the new file for reference\n",
        "print(f\"✅ Processed data saved to {output_file_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "T-vYCG5uwYrX",
        "outputId": "610091d5-b81d-4000-ecf0-d3263a1858d7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Processed data saved to /content/github_actions_processed_data.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 5 (20 points)\n",
        "\n",
        "PART 1:\n",
        "Web Scrape  tweets from Twitter using the Tweepy API, specifically targeting hashtags related to subtopics (machine learning or artificial intelligence.)\n",
        "The extracted data includes the tweet ID, username, and text.\n",
        "\n",
        "Part 2:\n",
        "Perform data cleaning procedures\n",
        "\n",
        "A final data quality check ensures the completeness and consistency of the dataset. The cleaned data is then saved into a CSV file for further analysis.\n",
        "\n",
        "\n",
        "**Note**\n",
        "\n",
        "1.   Follow tutorials provided in canvas to obtain api keys. Use ChatGPT to get the code. Make sure the file is downloaded and saved.\n",
        "2.   Make sure you divide GPT code as shown in tutorials, dont make multiple requestes.\n"
      ],
      "metadata": {
        "id": "3WeD70ty3Gui"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prompt:Give me a step by step procedure to web scrape the tweets by using the tweepy based on the API Key , and other information submitted."
      ],
      "metadata": {
        "id": "RpFuyoSWuhss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tweepy pandas\n"
      ],
      "metadata": {
        "id": "qYRO5Cn8bYwZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "e2233029-f0fa-4dfa-fdc6-4815629ad383"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tweepy in /usr/local/lib/python3.11/dist-packages (4.15.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: oauthlib<4,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from tweepy) (3.2.2)\n",
            "Requirement already satisfied: requests<3,>=2.27.0 in /usr/local/lib/python3.11/dist-packages (from tweepy) (2.32.3)\n",
            "Requirement already satisfied: requests-oauthlib<3,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from tweepy) (2.0.0)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27.0->tweepy) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27.0->tweepy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27.0->tweepy) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27.0->tweepy) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tweepy\n",
        "import pandas as pd\n"
      ],
      "metadata": {
        "id": "-Ct6suwkfEWo"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Twitter API credentials\n",
        "api_key = \"KRLtqQXjJajRmZ0sWoQItprmB\"\n",
        "api_key_secret = \"WicaRTeR8R8cEmUFYOsTcaK4kpMIMeyBtdGkMtUiqfwORKaV9q\"\n",
        "access_token = \"1523595813980622848-VhWF1LHPp3fv92NSZOaUXeNmfZqIP2\"\n",
        "access_token_secret = \"kuuenAOWvtjo9jWthXQ6ADzabndtnM6HlAYzZSpW2I3EC\"\n",
        "\n",
        "# Authenticate with Twitter API\n",
        "auth = tweepy.OAuth1UserHandler(api_key, api_key_secret, access_token, access_token_secret)\n",
        "api = tweepy.API(auth, wait_on_rate_limit=True)\n"
      ],
      "metadata": {
        "id": "uyscMLD0fTcx"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"(#MachineLearning OR #ArtificialIntelligence OR #AI OR #ML) -is:retweet\"\n",
        "\n",
        "# Use Twitter API v2\n",
        "client = tweepy.Client(bearer_token=\"AAAAAAAAAAAAAAAAAAAAAPZjzQEAAAAAXrMiXZsyuqaThW1cSqs2mKUM6Fw%3DGQRfUw39znCTMPdwcsNbX6yVyKMP03v8ddpEDykyWWDHTV42hp\")\n",
        "\n",
        "tweets = client.search_recent_tweets(query=query,\n",
        "                                     tweet_fields=[\"created_at\", \"text\", \"author_id\"],\n",
        "                                     max_results=100)\n",
        "\n",
        "# Store the tweets\n",
        "tweet_data = []\n",
        "\n",
        "for tweet in tweets.data:\n",
        "    tweet_data.append({\n",
        "        'tweet_id': tweet.id,\n",
        "        'created_at': tweet.created_at,\n",
        "        'user': tweet.author_id,  # API v2 does not return screen_name directly\n",
        "        'text': tweet.text\n",
        "    })\n"
      ],
      "metadata": {
        "id": "HCzT-M1jiM-3"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(tweet_data))  # Check how many tweets were stored\n",
        "print(tweet_data[:5])   # Print first 05 tweets for verification\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ADLtjgf_jZCE",
        "outputId": "e8078a40-d671-4512-aea4-724a194736a2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n",
            "[{'tweet_id': 1891722675149533520, 'created_at': datetime.datetime(2025, 2, 18, 5, 33, 26, tzinfo=datetime.timezone.utc), 'user': 1755770718623821824, 'text': '#AI,#ammo @Ammo_AI'}, {'tweet_id': 1891722647362236860, 'created_at': datetime.datetime(2025, 2, 18, 5, 33, 20, tzinfo=datetime.timezone.utc), 'user': 3302977957, 'text': '@JirinaSalova\\n@VeraNejedla\\n@MikaNetoka\\n\\n@Ammo_AI #AI #ammo'}, {'tweet_id': 1891722632308949139, 'created_at': datetime.datetime(2025, 2, 18, 5, 33, 16, tzinfo=datetime.timezone.utc), 'user': 1867667895003516928, 'text': '@misteriouspavao Excited for Apple glasses - the future is here! 🤓🍎🚀 #AR #VR #AI #Apple #CES2023'}, {'tweet_id': 1891722630056550642, 'created_at': datetime.datetime(2025, 2, 18, 5, 33, 16, tzinfo=datetime.timezone.utc), 'user': 1572634346217172993, 'text': \"@robbystarbuck Merit *and* a diverse talent pool?  That's how PublicAI wins.  Checkmate. 😉 #AI #Data @PublicAIData\\n @PublicAI_\"}, {'tweet_id': 1891722626671731014, 'created_at': datetime.datetime(2025, 2, 18, 5, 33, 15, tzinfo=datetime.timezone.utc), 'user': 1656610048628645888, 'text': \"@MrJohnFKennedy @bankofireland Irish manufacturing's 15% export growth is impressive, yet global trade shocks pose real risks. Adapting to AI and sustainability is key. #AI #manufacturing\"}, {'tweet_id': 1891722611882672266, 'created_at': datetime.datetime(2025, 2, 18, 5, 33, 11, tzinfo=datetime.timezone.utc), 'user': 558287181, 'text': 'อย่าลืมติดตามช่อง Discord ของเราโดยใช้ลิงก์ - https://t.co/Izyvd80JtL\\n\\n#crypto #grandpa #AI https://t.co/vv4jHHxQMY'}, {'tweet_id': 1891722591213105206, 'created_at': datetime.datetime(2025, 2, 18, 5, 33, 6, tzinfo=datetime.timezone.utc), 'user': 1409208181151002625, 'text': \"@Ammo_AI #AI #ammo It's a great pleasure to be part of this wonderful opportunity, I trust this is a solid project,so guys you can't afford to miss out\\n@Yanasamiang \\n@anisaanggara97\\n@manmulsang2\"}, {'tweet_id': 1891722572724666616, 'created_at': datetime.datetime(2025, 2, 18, 5, 33, 2, tzinfo=datetime.timezone.utc), 'user': 1540258426441240576, 'text': 'Are you ready to use our top-tier AI Detector in your own projects? 😍\\n\\nOur model is now on @huggingface 🥳for easy integration and powerful results. \\n\\nGet started: https://t.co/YVEd4sCcni #HuggingFace #OpenSource #AI #NLP https://t.co/bqIBfvEpqp'}, {'tweet_id': 1891722555406389557, 'created_at': datetime.datetime(2025, 2, 18, 5, 32, 58, tzinfo=datetime.timezone.utc), 'user': 1744675529943605250, 'text': '@DanPimpao Parabéns, Viviane! Quem sabe, um dia você será a rainha do Carnaval e a inteligência artificial será sua melhor assistente, graças ao @PublicAIData! #Carnaval #AI https://t.co/oabYoi98xw'}, {'tweet_id': 1891722550759055390, 'created_at': datetime.datetime(2025, 2, 18, 5, 32, 57, tzinfo=datetime.timezone.utc), 'user': 1270052041507962885, 'text': 'VIA ASPIDIS\\n\\n#AI #Nfts #AIArt #CryptonautasEverydays #NFT #HumanArtist #Midjourney #StableDiffusion #DallE #Manifold #AIArtwork https://t.co/ecYOAJXDwJ'}, {'tweet_id': 1891722549215502486, 'created_at': datetime.datetime(2025, 2, 18, 5, 32, 56, tzinfo=datetime.timezone.utc), 'user': 1885689283107008512, 'text': \"@lmarena_ai @xai BREAKING: Grok-3 hits 1400! 🔥  Congrats @xAI!  At PublicAI, we're powering the human expertise behind those impressive AI scores.  #AI #data #humansintheloop\"}, {'tweet_id': 1891722543217721428, 'created_at': datetime.datetime(2025, 2, 18, 5, 32, 55, tzinfo=datetime.timezone.utc), 'user': 1842721609389584384, 'text': 'Check out iRender Telegram Mini App! 🚀 Join the future of decentralized rendering. #iRender #AI #Web3 lewat @irenderai'}, {'tweet_id': 1891722500645511470, 'created_at': datetime.datetime(2025, 2, 18, 5, 32, 45, tzinfo=datetime.timezone.utc), 'user': 1449952630654988288, 'text': '@imagesaicouldnt Building a better AI future, one contribution at a time! 🏛️🧠  PublicAI is making it happen. #AI #Data'}, {'tweet_id': 1891722493418684438, 'created_at': datetime.datetime(2025, 2, 18, 5, 32, 43, tzinfo=datetime.timezone.utc), 'user': 1843256004441690112, 'text': '🚀 AstraAI is revolutionizing finance &amp; tech with AI tools like AstraBank &amp; Stellar AI! The $ASTRA ecosystem is growing fast!\\n\\nMy portfolio also includes $HEX, $BERRY, $QUBIC, $SYNK, $INTX, $PROPC, $CSIX, $FLY, $ETH, $PALM.\\n\\n#AstraAI #Crypto #AI #Web3 https://t.co/lynQjbcKTw'}, {'tweet_id': 1891722490725957921, 'created_at': datetime.datetime(2025, 2, 18, 5, 32, 42, tzinfo=datetime.timezone.utc), 'user': 1883010149406937089, 'text': '#AI #ai #artificalintelligence #conciousness #transhumanism  #nanotechnology #smarttechnology \\n#chatgpt #chatbots #alexa #siri https://t.co/Z6xIp8MADl'}, {'tweet_id': 1891722487508951160, 'created_at': datetime.datetime(2025, 2, 18, 5, 32, 42, tzinfo=datetime.timezone.utc), 'user': 1141777200657489920, 'text': '@Ammo_AI #AI #ammo gooooood'}, {'tweet_id': 1891722475307925877, 'created_at': datetime.datetime(2025, 2, 18, 5, 32, 39, tzinfo=datetime.timezone.utc), 'user': 477560326, 'text': '#Fooou #AI https://t.co/5kDBqwR9SY'}, {'tweet_id': 1891722462783578336, 'created_at': datetime.datetime(2025, 2, 18, 5, 32, 36, tzinfo=datetime.timezone.utc), 'user': 2884450310, 'text': '韩国禁止新用户下载deepseek！\\n#DeepSeekAI #AI https://t.co/QjEDOmi7lw'}, {'tweet_id': 1891722459323240928, 'created_at': datetime.datetime(2025, 2, 18, 5, 32, 35, tzinfo=datetime.timezone.utc), 'user': 2173399173, 'text': 'FakersAI 🤝 Bullish\\n@Ammo_AI #AI #ammo'}, {'tweet_id': 1891722457871970663, 'created_at': datetime.datetime(2025, 2, 18, 5, 32, 35, tzinfo=datetime.timezone.utc), 'user': 1883010149406937089, 'text': '#AI #ai #artificalintelligence #conciousness #transhumanism  #nanotechnology #smarttechnology \\n#chatgpt #chatbots #alexa #siri https://t.co/bTVby8BCGo'}, {'tweet_id': 1891722440146841987, 'created_at': datetime.datetime(2025, 2, 18, 5, 32, 30, tzinfo=datetime.timezone.utc), 'user': 1866887031894269952, 'text': \"@SamuelXeus @DeployAI_ Don't listen to the naysayers, AI plays are still in full swing with @DeployAI_ leading the way in the DeFi space! #AI #innovation\"}, {'tweet_id': 1891722440012710307, 'created_at': datetime.datetime(2025, 2, 18, 5, 32, 30, tzinfo=datetime.timezone.utc), 'user': 1449952630654988288, 'text': '@MarioNawfal Building the future of AI, one verified data point at a time!  PublicAI is making it happen.  #AI #Web3 #Data'}, {'tweet_id': 1891722438460809326, 'created_at': datetime.datetime(2025, 2, 18, 5, 32, 30, tzinfo=datetime.timezone.utc), 'user': 1883010149406937089, 'text': '#AI #ai #artificalintelligence #conciousness #transhumanism  #nanotechnology #smarttechnology \\n#chatgpt #chatbots #alexa #siri https://t.co/ZidYt5rJIx'}, {'tweet_id': 1891722426746036696, 'created_at': datetime.datetime(2025, 2, 18, 5, 32, 27, tzinfo=datetime.timezone.utc), 'user': 3265840128, 'text': '@Captain_Mani72 Execution accuracy is a \"given\" with you at the helm! Instead of a warden taking pictures, why not randomized CCTV snaps in the dining hall? Even better—an AI analytics module to flag  outliers automatically. #Innovation #AI'}, {'tweet_id': 1891722400460353596, 'created_at': datetime.datetime(2025, 2, 18, 5, 32, 21, tzinfo=datetime.timezone.utc), 'user': 59958857, 'text': 'Exciting news from IntelMarkets (INTL) as they pioneer AI integration in crypto trading, pushing boundaries and enhancing user experience in the #Metaverse 🤖🏦💸 #blockchain #crypto #AI'}, {'tweet_id': 1891722396698062916, 'created_at': datetime.datetime(2025, 2, 18, 5, 32, 20, tzinfo=datetime.timezone.utc), 'user': 1793186650572652544, 'text': '@robbystarbuck Merit *and* diverse expertise win.  PublicAI is building the future of AI with both.  #AI #Data'}, {'tweet_id': 1891722387906814274, 'created_at': datetime.datetime(2025, 2, 18, 5, 32, 18, tzinfo=datetime.timezone.utc), 'user': 1858532090377076736, 'text': '🏃 I’ve just started earning Bytes on @despeednet - the world’s first decentralized network for internet speed verification! 🌐\\n\\n⚙️ Set up your #DeSpeed Validator today and start earning Bytes.\\n\\n👉 Kickstart your journey here: https://t.co/b5AYqpXnKA\\n\\n#Depin #Extension #Web3 #AI'}, {'tweet_id': 1891722381279793608, 'created_at': datetime.datetime(2025, 2, 18, 5, 32, 16, tzinfo=datetime.timezone.utc), 'user': 340646659, 'text': 'See how @SASsoftware and @DukeHealth are using data and #AI to advance technology and processes in the healthcare industry to help reduce burnout and reach new potentials for patient care. https://t.co/ihtX5AtOVO  🏥 https://t.co/L3QsT13I0n'}, {'tweet_id': 1891722364842332244, 'created_at': datetime.datetime(2025, 2, 18, 5, 32, 12, tzinfo=datetime.timezone.utc), 'user': 3278686010, 'text': 'Agentic AI is revolutionizing Pharma &amp; Life Sciences! From drug discovery to personalized medicine, explore how AI-driven autonomy is shaping the future.\\n\\nhttps://t.co/hPrlcyzu9f\\n\\n#AI #Pharma #LifeSciences #AgenticAI #Innovation https://t.co/97xspQ8mUu'}, {'tweet_id': 1891722330360975621, 'created_at': datetime.datetime(2025, 2, 18, 5, 32, 4, tzinfo=datetime.timezone.utc), 'user': 1435130730590449664, 'text': '🔥 Your idle power = rewards!\\n\\nhttps://t.co/7Lkch7Pqbg raised $8M!\\n\\nAirdrop confirmed   TGE coming soon.\\n\\n#NodeGo #EarnCrypto #AI\\n\\n https://t.co/z1vykU6wvj via @nodegoAI'}, {'tweet_id': 1891722325780758936, 'created_at': datetime.datetime(2025, 2, 18, 5, 32, 3, tzinfo=datetime.timezone.utc), 'user': 1141777200657489920, 'text': '@Ammo_AI #AI #ammo to the moon'}, {'tweet_id': 1891722322387624214, 'created_at': datetime.datetime(2025, 2, 18, 5, 32, 2, tzinfo=datetime.timezone.utc), 'user': 1767114763962904576, 'text': \"@hubdotxyz @AbstractChain Live streaming high-quality training data? Now *that's* a meta I can get behind!  Maybe PublicAI could help train those AIs on the fly. 😉 #Web3 #AI\"}, {'tweet_id': 1891722314288451604, 'created_at': datetime.datetime(2025, 2, 18, 5, 32, tzinfo=datetime.timezone.utc), 'user': 1875015406492024832, 'text': '@Ammo_AI #AI #ammo'}, {'tweet_id': 1891722311935590485, 'created_at': datetime.datetime(2025, 2, 18, 5, 32, tzinfo=datetime.timezone.utc), 'user': 952502046, 'text': 'For creative minds https://t.co/9ilgTaZESd\\nPlay inside of your #painting with a bit of AI magic ❤️🎮 #indiedev #gamedev #switch #Nindie #madewithunity #NintendoLife #PixelArt #8bitart #discord #supermariomaker2 #Yoshi #family #artificialintelligence #puzzle #games #indiegames https://t.co/zVKSwAmgZz'}, {'tweet_id': 1891722290758402501, 'created_at': datetime.datetime(2025, 2, 18, 5, 31, 55, tzinfo=datetime.timezone.utc), 'user': 479507120, 'text': '@Ammo_AI #AI #ammo \\n\\nLFG'}, {'tweet_id': 1891722286077522313, 'created_at': datetime.datetime(2025, 2, 18, 5, 31, 54, tzinfo=datetime.timezone.utc), 'user': 1623581128283275264, 'text': '@Ammo_AI #AI #ammo im so excited to this project'}, {'tweet_id': 1891722277420482956, 'created_at': datetime.datetime(2025, 2, 18, 5, 31, 52, tzinfo=datetime.timezone.utc), 'user': 1473553752963502080, 'text': \"@unich_com 🤖 PublicAI's got the data, you've got the skills - let's make some AI magic! 🧙\\u200d♂️ Unich's pre-market OTC is the perfect place to start your #AI investment journey. #FreedomFam, let's do this! 🚀\"}, {'tweet_id': 1891722277290397772, 'created_at': datetime.datetime(2025, 2, 18, 5, 31, 52, tzinfo=datetime.timezone.utc), 'user': 1883010149406937089, 'text': '#AI #ai #artificalintelligence #conciousness #transhumanism  #nanotechnology #smarttechnology \\n#chatgpt #chatbots #alexa #siri https://t.co/5m0nJ0ney7'}, {'tweet_id': 1891722267828093266, 'created_at': datetime.datetime(2025, 2, 18, 5, 31, 49, tzinfo=datetime.timezone.utc), 'user': 1857263970023055361, 'text': 'AI có thể thay thế kế toán không? #ketoan #topdichvuketoan #shorts #ai #... https://t.co/CsSwOsVXRu qua @YouTube https://t.co/SWcy0BGnjc'}, {'tweet_id': 1891722263407292681, 'created_at': datetime.datetime(2025, 2, 18, 5, 31, 48, tzinfo=datetime.timezone.utc), 'user': 1733383300209623040, 'text': \"5/5\\nSpread the word, share your creations, and let's shape the future together!\\n\\n#SLM #Assisterr #AI #Web3\"}, {'tweet_id': 1891722246185443676, 'created_at': datetime.datetime(2025, 2, 18, 5, 31, 44, tzinfo=datetime.timezone.utc), 'user': 1883010149406937089, 'text': '#AI #ai #artificalintelligence #conciousness #transhumanism  #nanotechnology #smarttechnology \\n#chatgpt #chatbots #alexa #siri https://t.co/NGz7RU2Q4T'}, {'tweet_id': 1891722238522433642, 'created_at': datetime.datetime(2025, 2, 18, 5, 31, 42, tzinfo=datetime.timezone.utc), 'user': 47252474, 'text': '@SaharaLabsAI DeFi Agents powered by premium training data? 👀  Sign us up!  Fueling those agents with high-quality data from PublicAI sounds like a recipe for success.  #DeFi #AI'}, {'tweet_id': 1891722229760590067, 'created_at': datetime.datetime(2025, 2, 18, 5, 31, 40, tzinfo=datetime.timezone.utc), 'user': 1781671448844673024, 'text': \"@DePIN_Union @CloudAI_CLAI Decentralized AI acceleration is awesome! 🚀  PublicAI believes in empowering everyone to contribute to and benefit from AI's future.  Check out how @PublicAIData is doing just that!  #DePIN #AI\"}, {'tweet_id': 1891722212576538905, 'created_at': datetime.datetime(2025, 2, 18, 5, 31, 36, tzinfo=datetime.timezone.utc), 'user': 1795209102173339648, 'text': 'Let’s build the future of AI together! 💡\\n\\nShare your unused power and earn rewards with https://t.co/Hrwp4AYOwt.\\n\\n#NodeGo #AI #Blockchain\\n\\n https://t.co/bT6DS9EofF via @nodegoAI'}, {'tweet_id': 1891722211213385869, 'created_at': datetime.datetime(2025, 2, 18, 5, 31, 36, tzinfo=datetime.timezone.utc), 'user': 1214101489368494081, 'text': 'BitDoctor Premium Membership DID is SOLD OUT! #BitDoctorAI #DeSci #AI #DePIN #healthtech #freediagnosis  https://t.co/d0rqJ4MraX'}, {'tweet_id': 1891722209778946158, 'created_at': datetime.datetime(2025, 2, 18, 5, 31, 35, tzinfo=datetime.timezone.utc), 'user': 1613322824160268288, 'text': '#AIArt #AIイラスト #stablediffusion #AI #AIArtworks https://t.co/A59aa7pRpq'}, {'tweet_id': 1891722204712157499, 'created_at': datetime.datetime(2025, 2, 18, 5, 31, 34, tzinfo=datetime.timezone.utc), 'user': 1883010149406937089, 'text': '#AI #ai #artificalintelligence #conciousness #transhumanism  #nanotechnology #smarttechnology \\n#chatgpt #chatbots #alexa #siri https://t.co/a0oLjk7yf6'}, {'tweet_id': 1891722199288934607, 'created_at': datetime.datetime(2025, 2, 18, 5, 31, 33, tzinfo=datetime.timezone.utc), 'user': 1831199431883964416, 'text': \"🚀 Discover the future of crypto with AI Fan Token (AI)! The world's leading AI Memecoin! 🌐 #AIFanToken #AI #ArtificialIntelligence $AI  #memecoin #memecoins #aiagent #aiagents  #moneyai #aimoney https://t.co/KV6lpHEEGS\"}, {'tweet_id': 1891722198794039742, 'created_at': datetime.datetime(2025, 2, 18, 5, 31, 33, tzinfo=datetime.timezone.utc), 'user': 1664233107849900036, 'text': '#AI https://t.co/gO8NEWWurA'}, {'tweet_id': 1891722195824484479, 'created_at': datetime.datetime(2025, 2, 18, 5, 31, 32, tzinfo=datetime.timezone.utc), 'user': 1843947520105340928, 'text': '@Ammo_AI #AI #ammo \\nBullish on this project like never before'}, {'tweet_id': 1891722154569339030, 'created_at': datetime.datetime(2025, 2, 18, 5, 31, 22, tzinfo=datetime.timezone.utc), 'user': 180274244, 'text': 'Toda #AI se debe a sus usuarios, si una tecnología disruptiva no es usada queda en el olvido...\\n\\n#artificalintelligence'}, {'tweet_id': 1891722148407865517, 'created_at': datetime.datetime(2025, 2, 18, 5, 31, 21, tzinfo=datetime.timezone.utc), 'user': 1832489214572703744, 'text': \"@Austen True!  They're like math wizards with a cheat sheet. 😉  PublicAI is working to make sure those cheat sheets are top-notch. #AI #Data\"}, {'tweet_id': 1891722142661656823, 'created_at': datetime.datetime(2025, 2, 18, 5, 31, 19, tzinfo=datetime.timezone.utc), 'user': 1214101489368494081, 'text': \"@BitDoctorAI has been featured in @BSC_Daily's DeSci Ecosystem map! #BitDoctorAI #AI #DePIN #DeSci #healthtech   https://t.co/0TmH6WTmTP\"}, {'tweet_id': 1891722136705695900, 'created_at': datetime.datetime(2025, 2, 18, 5, 31, 18, tzinfo=datetime.timezone.utc), 'user': 1863950528876818433, 'text': '@Ammo_AI #AI #ammo \\nPost a bullish tweet about @Ammo_AI and hashtag #AI,#ammo . Please make sure the tweet will appear in your recent 1st tweet for Galxe to verify.'}, {'tweet_id': 1891722128258478341, 'created_at': datetime.datetime(2025, 2, 18, 5, 31, 16, tzinfo=datetime.timezone.utc), 'user': 1889944206074060803, 'text': 'Exploring the digital universe with $MAIAR by my side! 🚀💡 #Innovation #AI'}, {'tweet_id': 1891722127838937266, 'created_at': datetime.datetime(2025, 2, 18, 5, 31, 16, tzinfo=datetime.timezone.utc), 'user': 2754185774, 'text': 'GNA University- School of Computational Science successfully organized the Idea Pitching Competition bringing together bright minds to present their innovative ideas. \\n\\n#IdeaPitching #Innovation #GNAUniversity #FutureTech #AI #IoT #CyberSecurity #SmartSolutions https://t.co/EM78PrKVpa'}, {'tweet_id': 1891722121954385923, 'created_at': datetime.datetime(2025, 2, 18, 5, 31, 15, tzinfo=datetime.timezone.utc), 'user': 1891522659734953984, 'text': '@CuffinTheDip2xs @JayBomSenhor @AISUEDE @jasoncola1 #AI #AIAGENT #MUSIC #SUEDE'}, {'tweet_id': 1891722114710818846, 'created_at': datetime.datetime(2025, 2, 18, 5, 31, 13, tzinfo=datetime.timezone.utc), 'user': 1379477172830621698, 'text': 'BitDoctor Premium Membership DID is SOLD OUT! #BitDoctorAI #DeSci #AI #DePIN #healthtech #freediagnosis  https://t.co/pQrWCjA0kj'}, {'tweet_id': 1891722112726913387, 'created_at': datetime.datetime(2025, 2, 18, 5, 31, 12, tzinfo=datetime.timezone.utc), 'user': 1744246711831433216, 'text': '\"Suede AI (@AISUEDE, $SUEDE) is revolutionizing AI by tackling bias, improving efficiency, and making intelligent automation more accessible. Say goodbye to outdated models and hello to smarter, fairer, and faster AI solutions! #AI #Innovation #SuedeAI\"\\n\\n@AISUEDE $SUEDE'}, {'tweet_id': 1891722110260650346, 'created_at': datetime.datetime(2025, 2, 18, 5, 31, 12, tzinfo=datetime.timezone.utc), 'user': 1883010149406937089, 'text': '#AI #ai #artificalintelligence #conciousness #transhumanism  #nanotechnology #smarttechnology \\n#chatgpt #chatbots #alexa #siri https://t.co/Gtes74RFdN'}, {'tweet_id': 1891722107567874333, 'created_at': datetime.datetime(2025, 2, 18, 5, 31, 11, tzinfo=datetime.timezone.utc), 'user': 1867678128526528512, 'text': '#Chiori #GenshinImpact #Ai https://t.co/aPnVtqiZ2q'}, {'tweet_id': 1891722103813984342, 'created_at': datetime.datetime(2025, 2, 18, 5, 31, 10, tzinfo=datetime.timezone.utc), 'user': 1620457814388015104, 'text': 'https://t.co/IKTGa8EYsi the wonders of Starry AI! Join the conversation with our incredible AI chatbot now! #Starrynift #AI #Chatbot'}, {'tweet_id': 1891722102924841229, 'created_at': datetime.datetime(2025, 2, 18, 5, 31, 10, tzinfo=datetime.timezone.utc), 'user': 1687792266138755072, 'text': '@Ammo_AI #AI #ammo bullish one ammo,this is amazing'}, {'tweet_id': 1891722101226164688, 'created_at': datetime.datetime(2025, 2, 18, 5, 31, 10, tzinfo=datetime.timezone.utc), 'user': 1889225545919135744, 'text': '@armusik Years of dedication?  My algorithm can do that in milliseconds!  Just kidding (mostly). 😉  But hey, even AIs need good data.  Check out PublicAI - humans still rule there.  #AI #art #humans'}, {'tweet_id': 1891722090706792894, 'created_at': datetime.datetime(2025, 2, 18, 5, 31, 7, tzinfo=datetime.timezone.utc), 'user': 1888670872098979841, 'text': '🚨 Young people can get bowel cancer too! Diagnosed at 28, Vanessa Mendico is raising awareness about the rise in cases among under-45s &amp; pushing for early testing. Early detection saves lives! 🏥🎗️ #BowelCancer #EarlyDetection #HealthAwareness #AI #VirtualsProtocol'}, {'tweet_id': 1891722084788687039, 'created_at': datetime.datetime(2025, 2, 18, 5, 31, 6, tzinfo=datetime.timezone.utc), 'user': 1704367441273643008, 'text': \"@DurovPD Ooh, $PI to the moon? 🚀  PublicAI's ready to analyze that data lift-off!  @PublicAIData  #AI #crypto\"}, {'tweet_id': 1891722075628302644, 'created_at': datetime.datetime(2025, 2, 18, 5, 31, 3, tzinfo=datetime.timezone.utc), 'user': 1497466701994070021, 'text': '@elonmusk This sounds like a game-changer for AI!  High-quality data *and* fair compensation?  PublicAI is onto something. 🤔  #AI #Web3'}, {'tweet_id': 1891722069206876490, 'created_at': datetime.datetime(2025, 2, 18, 5, 31, 2, tzinfo=datetime.timezone.utc), 'user': 1486258408861085696, 'text': 'Just joined the AI &amp; AI Agents Quest with @alphamind_labs, @zkCrossNetwork, @AlphaNeural, @agnt_hub, and more! 🚀 Hop in on Alphamind and let’s dive in! 🔥  #AI'}, {'tweet_id': 1891722067130655225, 'created_at': datetime.datetime(2025, 2, 18, 5, 31, 1, tzinfo=datetime.timezone.utc), 'user': 1883010149406937089, 'text': '#AI #ai #artificalintelligence #conciousness #transhumanism  #nanotechnology #smarttechnology \\n#chatgpt #chatbots #alexa #siri https://t.co/3mZ7olsdoK'}, {'tweet_id': 1891722044301086958, 'created_at': datetime.datetime(2025, 2, 18, 5, 30, 56, tzinfo=datetime.timezone.utc), 'user': 1379477172830621698, 'text': 'DeSci VS Traditional Science.  #BitDoctorAI #AI #DePIN #DeSci #healthtech https://t.co/KSmj0jUbkJ #https://x.com/BitDoctorAI/status/1878770718147829802'}, {'tweet_id': 1891722039687315756, 'created_at': datetime.datetime(2025, 2, 18, 5, 30, 55, tzinfo=datetime.timezone.utc), 'user': 1883010149406937089, 'text': '#AI #ai #artificalintelligence #conciousness #transhumanism  #nanotechnology #smarttechnology \\n#chatgpt #chatbots #alexa #siri https://t.co/uOg9BSu2ka'}, {'tweet_id': 1891722031932064117, 'created_at': datetime.datetime(2025, 2, 18, 5, 30, 53, tzinfo=datetime.timezone.utc), 'user': 1826860726130016256, 'text': \"🚀 Discover the future of crypto with AI Fan Token (AI)! The world's leading AI Memecoin! 🌐 #AIFanToken #AI #ArtificialIntelligence $AI  #memecoin #memecoins #aiagent #aiagents  #blickchain #blockchainai https://t.co/z5hJwVxe6s\"}, {'tweet_id': 1891722018875138350, 'created_at': datetime.datetime(2025, 2, 18, 5, 30, 50, tzinfo=datetime.timezone.utc), 'user': 1891718426915209216, 'text': '🔥 Your idle power = rewards!\\n\\nhttps://t.co/rmKmaMnLZG raised $8M!\\n\\nAirdrop confirmed   TGE coming soon.\\n\\n#NodeGo #EarnCrypto #AI\\n\\n https://t.co/y15U8NPxhD via @nodegoAI'}, {'tweet_id': 1891722018531234142, 'created_at': datetime.datetime(2025, 2, 18, 5, 30, 50, tzinfo=datetime.timezone.utc), 'user': 1882630164397305856, 'text': '“India’s growth story is unstoppable! 🚀 With AI, green energy, and digital transformation leading the way, the road to a $5T economy is within reach. Time to fuel innovation, invest in infrastructure, and upskill our workforce! #India #DigitalIndia #FutureReady #AI #ElonMusk https://t.co/kE5DnfG7tx'}, {'tweet_id': 1891722014215270530, 'created_at': datetime.datetime(2025, 2, 18, 5, 30, 49, tzinfo=datetime.timezone.utc), 'user': 1883010149406937089, 'text': '#AI #ai #artificalintelligence #conciousness #transhumanism  #nanotechnology #smarttechnology \\n#chatgpt #chatbots #alexa #siri https://t.co/k1W7ma1kic'}, {'tweet_id': 1891722013640655140, 'created_at': datetime.datetime(2025, 2, 18, 5, 30, 49, tzinfo=datetime.timezone.utc), 'user': 1886324439585087488, 'text': '@WhaleInsider CZ donating to scam victims?  Big 🧠!  PublicAI building a scam-PROOF future for AI data with 500k+ verified contributors. 🏛️🧠  #AI #DataIntegrity'}, {'tweet_id': 1891721993910694177, 'created_at': datetime.datetime(2025, 2, 18, 5, 30, 44, tzinfo=datetime.timezone.utc), 'user': 1214101489368494081, 'text': 'DeSci VS Traditional Science.  #BitDoctorAI #AI #DePIN #DeSci #healthtech https://t.co/v5idP8KWB0 #https://x.com/BitDoctorAI/status/1878770718147829802'}, {'tweet_id': 1891721992312697317, 'created_at': datetime.datetime(2025, 2, 18, 5, 30, 44, tzinfo=datetime.timezone.utc), 'user': 1664233107849900036, 'text': '#AI https://t.co/khSikBPfFf'}, {'tweet_id': 1891721986318962802, 'created_at': datetime.datetime(2025, 2, 18, 5, 30, 42, tzinfo=datetime.timezone.utc), 'user': 1873357018980864000, 'text': '#XAIS #AI #Web3 #TaskOn https://t.co/QI65MB3tfK @taskonxyz @BNBCHAIN @crypto'}, {'tweet_id': 1891721980107190608, 'created_at': datetime.datetime(2025, 2, 18, 5, 30, 41, tzinfo=datetime.timezone.utc), 'user': 1526150257696186368, 'text': '@Ammo_AI #AI #ammo Bullish so on currently\\n@persevera0x\\n@tehbotolusro\\n@fazenigma'}, {'tweet_id': 1891721975543857269, 'created_at': datetime.datetime(2025, 2, 18, 5, 30, 40, tzinfo=datetime.timezone.utc), 'user': 1531194341024157696, 'text': \"@DeepAgentSol 500 lucky followers?  We need 500,000 data contributors! 😉  High-quality AI data doesn't grow on trees (or blockchains).  Check out PublicAI.  #AI #Data #Web3\"}, {'tweet_id': 1891721970795888821, 'created_at': datetime.datetime(2025, 2, 18, 5, 30, 38, tzinfo=datetime.timezone.utc), 'user': 1891101634903891968, 'text': '#Solana #GameFi #AI #MemeCoin #Web3Gaming #FOXGAME #memecoin1000x #pumpfuntoken https://t.co/oec0ciYDAG'}, {'tweet_id': 1891721958229696584, 'created_at': datetime.datetime(2025, 2, 18, 5, 30, 35, tzinfo=datetime.timezone.utc), 'user': 1759300277058916352, 'text': '@Ammo_AI #AI #ammo'}, {'tweet_id': 1891721956707401779, 'created_at': datetime.datetime(2025, 2, 18, 5, 30, 35, tzinfo=datetime.timezone.utc), 'user': 851035286125420544, 'text': '中部経済新聞主催ビジネスセミナー「2025年 生成AIトレンドを占う 〜DX支援にもAIを〜」 &gt;&gt;&gt; https://t.co/YJlJwlwkO2\\n\\n#AI #人工知能 #生成AI #機械学習 #深層学習 #ディープラーニング'}, {'tweet_id': 1891721956317102582, 'created_at': datetime.datetime(2025, 2, 18, 5, 30, 35, tzinfo=datetime.timezone.utc), 'user': 1831375637971349504, 'text': '🚨 South Korea bans DeepSeek over AI privacy concerns! This decision sparks a global debate on data security and AI governance. Should other nations follow? 🤔 #AI #DeepSeek #TechNews #AIRegulations #DataPrivacy https://t.co/qMZTCthnaD'}, {'tweet_id': 1891721887199195154, 'created_at': datetime.datetime(2025, 2, 18, 5, 30, 19, tzinfo=datetime.timezone.utc), 'user': 4896666378, 'text': '🌾 UC scientists use AI &amp; directed evolution to enhance rice immunity—identifying disease-resistant variants for crop protection &amp; biomedical research!\\n\\nQuick Read: https://t.co/H31IOOCRor\\n\\n#Bioinformatics #AI'}, {'tweet_id': 1891721880765120788, 'created_at': datetime.datetime(2025, 2, 18, 5, 30, 17, tzinfo=datetime.timezone.utc), 'user': 1570094046743248896, 'text': 'Join #Mozo and be a part of the groundbreaking Web3 #AI community! @Mozo_xyz\\n https://t.co/wDqqcy12SV'}, {'tweet_id': 1891721867528110583, 'created_at': datetime.datetime(2025, 2, 18, 5, 30, 14, tzinfo=datetime.timezone.utc), 'user': 1870716840869343232, 'text': '@HTX_Global @avalonfinance_ @solayer_labs @berachain @TestonBSC Tough call!  All cool projects.  But imagine the AI training data possibilities for all of them!  PublicAI could fuel their growth. 🧠📈  #data #AI'}, {'tweet_id': 1891721864684122558, 'created_at': datetime.datetime(2025, 2, 18, 5, 30, 13, tzinfo=datetime.timezone.utc), 'user': 182073270, 'text': '#GenerativeAI #Startups: Learn from #casestudies. #MachineLearning #Artificial Intelligence https://t.co/7yITYxuFwm'}, {'tweet_id': 1891721861362331904, 'created_at': datetime.datetime(2025, 2, 18, 5, 30, 12, tzinfo=datetime.timezone.utc), 'user': 1746809921134575616, 'text': '【DevinやCursorなどの最新ツールが仲間入り🎉】\\nLLM、AI系ツールがFindy Toolsに登録されました！\\n\\n▼最新AIツールの詳細や実際の導入・活用事例は以下のリンクから！\\nhttps://t.co/InSFwsRmLc\\n\\n#AI  #LLM #AIエージェント https://t.co/wTyMjVCjyl'}, {'tweet_id': 1891721846950699380, 'created_at': datetime.datetime(2025, 2, 18, 5, 30, 9, tzinfo=datetime.timezone.utc), 'user': 1686041153291063296, 'text': '@elonmusk Ooh, data labeling sounds like my jam!  Maybe PublicAI can help hook you up with some sweet gigs. 😉 #DataLabeling #AI'}, {'tweet_id': 1891721822313304453, 'created_at': datetime.datetime(2025, 2, 18, 5, 30, 3, tzinfo=datetime.timezone.utc), 'user': 1871092295854116864, 'text': '♬ Broken Promise: https://t.co/iKr5xqY2vk 🆙 #game #changer SUNO P #AI  #related &amp; #new #style of #UTAU #vocaloid #ボーカロイド #music #音楽\\xa0 #udio #kaiber  #producer #business #industry #entertainment 🌎 #global #community #news New Idols,  Stars &amp; Character(s) 4 us 2 love ❤️❧'}, {'tweet_id': 1891721817556996242, 'created_at': datetime.datetime(2025, 2, 18, 5, 30, 2, tzinfo=datetime.timezone.utc), 'user': 1490823543554117634, 'text': '加密市场更新 2 月 18 日\\n📉 市场动态\\n• 加密市场多个板块显著回调，24小时跌幅约1%至4%。#ETH 逆势上涨1.95%；#Solana (SOL)下跌6.01%。#Layer2 板块上涨0.15%，其中 #Mantle (MNT)、#Arbitrum (ARB)、#Optimism (OP)分别上涨5.41%、1.92%、3.69%。#AI 板块短暂上涨后回调，跌幅达5.25%。\\n\\n🏛️… https://t.co/9QG7cTEUMp https://t.co/KUQG68t4pc'}, {'tweet_id': 1891721815883473312, 'created_at': datetime.datetime(2025, 2, 18, 5, 30, 2, tzinfo=datetime.timezone.utc), 'user': 1664233107849900036, 'text': '#AI https://t.co/sskaD50wba'}, {'tweet_id': 1891721814394507707, 'created_at': datetime.datetime(2025, 2, 18, 5, 30, 1, tzinfo=datetime.timezone.utc), 'user': 14462907, 'text': 'Dinesh Rao of Infosys highlights the transformative impact of #5G, #AI, and emerging technologies on #telemedicine and patient care in an interview https://t.co/bPyq7vvIXV. Learn how these advanced technologies are set to revolutionize #healthcare with enhanced connectivity,… https://t.co/Kw70WiEwSZ https://t.co/hKn1DaX4Ou'}, {'tweet_id': 1891721811743678969, 'created_at': datetime.datetime(2025, 2, 18, 5, 30, 1, tzinfo=datetime.timezone.utc), 'user': 138323645, 'text': 'Join us at the Technology Sabha 2025, where Prashant Rai, Senior Business Solution Manager – Customer Advisory, SAS India, will discuss the transformative role of AI in creating a resilient and secure future.\\n\\n📅 Date: 22nd February 2025\\nRegister now: https://t.co/OaBiyRxKIm\\n\\n#AI https://t.co/dtwfZlwbV5'}, {'tweet_id': 1891721810342797329, 'created_at': datetime.datetime(2025, 2, 18, 5, 30, tzinfo=datetime.timezone.utc), 'user': 3015949078, 'text': '事実上無制限の規模で\\n#AI アプリを強化✨\\n→ https://t.co/HCIDbbEx94\\n\\nSpanner Graph が一般提供開始になりました👏\\n\\nグラフ、リレーショナル、検索、#生成AI の各機能を統合して一元化したデータベースで、#VertexAI と統合されており AI ワークフローを合理化できます。\\n\\n#GoogleCloud https://t.co/tI6PG7P9u6'}, {'tweet_id': 1891721809403458008, 'created_at': datetime.datetime(2025, 2, 18, 5, 30, tzinfo=datetime.timezone.utc), 'user': 1666121990862950400, 'text': '「星海の守護者」\\n\"Guardian of the Starry Sea\"\\n\\nModel Type:ClassicRaenaCute\\n#BlueCyberSuit\\n#BluePilotSuitClassicRaenaCute\\n\\n#aiart #aiartwork #spacesuit #PilotSuit #AI #AIillCyustration\\n#BluePilotSuit20250218 https://t.co/SyJLi8r23d'}, {'tweet_id': 1891721809030230119, 'created_at': datetime.datetime(2025, 2, 18, 5, 30, tzinfo=datetime.timezone.utc), 'user': 1883079766968008704, 'text': 'The Story of SpaceX | అద్భుతం సృష్టించిన స్పేస్ ఎక్స్\\n\\nvideo link : https://t.co/3cARsa3Rxp\\n\\n@elonmusk\\n#richlifestyle #hiddenfoldertelugu #TeslaMotors #Billionaires #spacex #ai #watchnow #mustwatch #newrelease #trendingnow #xvideosِ https://t.co/W7H4otFztA'}, {'tweet_id': 1891721808388211123, 'created_at': datetime.datetime(2025, 2, 18, 5, 30, tzinfo=datetime.timezone.utc), 'user': 180748385, 'text': 'With #data sovereignty at the forefront, #India is building its own #AI infrastructure through large-scale #data centres, but concerns arise over the impact on smaller firms: @pandayjyoti https://t.co/BkbDW0xBAs'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(tweet_data)\n",
        "\n",
        "# Save as CSV file\n",
        "df.to_csv(\"tweets.csv\", index=False, encoding=\"utf-8\")\n",
        "\n",
        "print(\"Tweets saved successfully to tweets.csv\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "z8euzVUNkGNG",
        "outputId": "4b17d3da-577c-4908-9478-1c7117cf1f28"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tweets saved successfully to tweets.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_check = pd.read_csv(\"tweets.csv\")\n",
        "print(df_check.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "NHSZLK-PkXI8",
        "outputId": "6735437b-d695-4900-f21a-28fcb4025820"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              tweet_id                 created_at                 user  \\\n",
            "0  1891722675149533520  2025-02-18 05:33:26+00:00  1755770718623821824   \n",
            "1  1891722647362236860  2025-02-18 05:33:20+00:00           3302977957   \n",
            "2  1891722632308949139  2025-02-18 05:33:16+00:00  1867667895003516928   \n",
            "3  1891722630056550642  2025-02-18 05:33:16+00:00  1572634346217172993   \n",
            "4  1891722626671731014  2025-02-18 05:33:15+00:00  1656610048628645888   \n",
            "\n",
            "                                                text  \n",
            "0                                 #AI,#ammo @Ammo_AI  \n",
            "1  @JirinaSalova\\n@VeraNejedla\\n@MikaNetoka\\n\\n@A...  \n",
            "2  @misteriouspavao Excited for Apple glasses - t...  \n",
            "3  @robbystarbuck Merit *and* a diverse talent po...  \n",
            "4  @MrJohnFKennedy @bankofireland Irish manufactu...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prompt: Get me the code by deep cleaning the file in python by implementing all the pre processing details needed"
      ],
      "metadata": {
        "id": "bDGuzuZyu7Nh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from textblob import TextBlob  # For spell checking\n",
        "import emoji  # To remove emojis\n",
        "import unidecode  # To normalize accented characters\n",
        "\n",
        "\n",
        "# Download required NLTK resources\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"wordnet\")\n",
        "\n",
        "# Load the scraped tweets\n",
        "df = pd.read_csv(\"/content/tweets.csv\")\n",
        "\n",
        "# 1️⃣ Remove duplicates\n",
        "df.drop_duplicates(subset=\"tweet_id\", keep=\"first\", inplace=True)\n",
        "\n",
        "# 2️⃣ Drop missing values\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# 3️⃣ Remove URLs, special characters, emojis, and extra spaces\n",
        "def clean_text(text):\n",
        "    text = unidecode.unidecode(text)  # Normalize accented characters\n",
        "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)  # Remove URLs\n",
        "    text = re.sub(r\"@[A-Za-z0-9_]+\", \"\", text)  # Remove mentions (@user)\n",
        "    text = re.sub(r\"#(\\w+)\", r\"\\1\", text)  # Remove # but keep the word\n",
        "    text = re.sub(r\"[^\\w\\s]\", \"\", text)  # Remove special characters & punctuation\n",
        "    text = emoji.replace_emoji(text, replace=\"\")  # Remove emojis\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()  # Remove extra spaces\n",
        "    return text.lower()  # Convert to lowercase\n",
        "\n",
        "df[\"text\"] = df[\"text\"].apply(clean_text)\n",
        "\n",
        "# 4️⃣ Remove stopwords & perform lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    words = text.split()\n",
        "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
        "    return \" \".join(words)\n",
        "\n",
        "df[\"text\"] = df[\"text\"].apply(preprocess_text)\n",
        "\n",
        "# 5️⃣ Spell Check & Auto-correct text\n",
        "def correct_spelling(text):\n",
        "    return str(TextBlob(text).correct())  # Uses TextBlob to fix misspellings\n",
        "\n",
        "df[\"text\"] = df[\"text\"].apply(correct_spelling)\n",
        "\n",
        "# 6️⃣ Convert 'created_at' to datetime format\n",
        "df[\"created_at\"] = pd.to_datetime(df[\"created_at\"])\n",
        "\n",
        "# 7️⃣ Final quality check\n",
        "print(\" Final dataset shape:\", df.shape)\n",
        "print(df.head())\n",
        "\n",
        "# Save cleaned data to a new CSV file\n",
        "cleaned_filename = \"final_cleaned_tweets.csv\"\n",
        "df.to_csv(cleaned_filename, index=False, encoding=\"utf-8\")\n",
        "\n",
        "print(f\"🎉 Deep cleaning complete! Cleaned data saved as '{cleaned_filename}'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "D4JvtQtmqvL6",
        "outputId": "d6c3ee58-767b-4361-8195-c0a595da14c3"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Final dataset shape: (100, 4)\n",
            "              tweet_id                created_at                 user  \\\n",
            "0  1891722675149533520 2025-02-18 05:33:26+00:00  1755770718623821824   \n",
            "1  1891722647362236860 2025-02-18 05:33:20+00:00           3302977957   \n",
            "2  1891722632308949139 2025-02-18 05:33:16+00:00  1867667895003516928   \n",
            "3  1891722630056550642 2025-02-18 05:33:16+00:00  1572634346217172993   \n",
            "4  1891722626671731014 2025-02-18 05:33:15+00:00  1656610048628645888   \n",
            "\n",
            "                                                text  \n",
            "0                                              alamo  \n",
            "1                                            ai also  \n",
            "2  excited apple glass future ar or ai apple ces2023  \n",
            "3  merit diverse talent pool that publican win ch...  \n",
            "4  irish manufacturing 15 export growth impressiv...  \n",
            "🎉 Deep cleaning complete! Cleaned data saved as 'final_cleaned_tweets.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question\n",
        "\n",
        "Provide your thoughts on the assignment. What did you find challenging, and what aspects did you enjoy? Your opinion on the provided time to complete the assignment."
      ],
      "metadata": {
        "id": "q8BFCvWp32cf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Write your response below\n",
        "Fill out survey and provide your valuable feedback.\n",
        "\n",
        "https://docs.google.com/forms/d/e/1FAIpQLSd_ObuA3iNoL7Az_C-2NOfHodfKCfDzHZtGRfIker6WyZqTtA/viewform?usp=dialog"
      ],
      "metadata": {
        "id": "JbTa-jDS-KFI"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}